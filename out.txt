rows: 28, cols: 28
Number of training images: 60000
rows: 28, cols: 28
Number of test images: 10000
model layers = 0
Model created with 2 layers
Layer 0: 784 inputs, 8 neurons
Layer 1: 8 inputs, 10 neurons
Number of parameters: 6370
Batch size: 16
Error opening directory /home/rickojn/coding/cf-mlp/models/No model found, training from scratch
matmul simd ....
Time spent in matmul_simd_forward: 0.159394 seconds
relu forward ...
Time spent in relu_forward: 0.000185 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.002384 seconds
softmax forward ...
Time spent in softmax_forward: 0.000776 seconds
Test loss before training: 2.331046
Test accuracy before training: 0.099500


training loop:

epoch: 0
matmul simd ....
Time spent in matmul_simd_forward: 0.000106 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000006 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.010159
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.317282
Training accuracy: 0.000000

epoch: 1
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005313
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.347926
Training accuracy: 0.187500

epoch: 2
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.009717
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.318609
Training accuracy: 0.062500

epoch: 3
matmul simd ....
Time spent in matmul_simd_forward: 0.000104 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.008192
relu backward ...
Time spent in relu_backward: 0.000018 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.001987


Training loss: 2.326805
Training accuracy: 0.000000

epoch: 4
matmul simd ....
Time spent in matmul_simd_forward: 0.000090 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000003 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.018685
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000065 seconds
Layer 0 weight grad [0][0] = 0.041948


Training loss: 2.299843
Training accuracy: 0.125000

epoch: 5
matmul simd ....
Time spent in matmul_simd_forward: 0.000110 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.007605
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000065 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.357962
Training accuracy: 0.000000

epoch: 6
matmul simd ....
Time spent in matmul_simd_forward: 0.000083 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005056
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000100 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.338141
Training accuracy: 0.000000

epoch: 7
matmul simd ....
Time spent in matmul_simd_forward: 0.000121 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005052
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.320931
Training accuracy: 0.125000

epoch: 8
matmul simd ....
Time spent in matmul_simd_forward: 0.000098 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.010230
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000131 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.315917
Training accuracy: 0.250000

epoch: 9
matmul simd ....
Time spent in matmul_simd_forward: 0.000085 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = -0.043764
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.339670
Training accuracy: 0.000000

epoch: 10
matmul simd ....
Time spent in matmul_simd_forward: 0.000103 seconds
relu forward ...
Time spent in relu_forward: 0.000004 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.005458
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000087 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.333783
Training accuracy: 0.000000

epoch: 11
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.004938
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000086 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.391696
Training accuracy: 0.062500

epoch: 12
matmul simd ....
Time spent in matmul_simd_forward: 0.000105 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.005200
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000065 seconds
Layer 0 weight grad [0][0] = 0.000944


Training loss: 2.375626
Training accuracy: 0.000000

epoch: 13
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000003 seconds
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.009921
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000090 seconds
Layer 0 weight grad [0][0] = -0.010229


Training loss: 2.192645
Training accuracy: 0.375000

epoch: 14
matmul simd ....
Time spent in matmul_simd_forward: 0.000098 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005959
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000065 seconds
Layer 0 weight grad [0][0] = 0.000901


Training loss: 2.255935
Training accuracy: 0.125000

epoch: 15
matmul simd ....
Time spent in matmul_simd_forward: 0.000107 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005505
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000103 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.412710
Training accuracy: 0.000000

epoch: 16
matmul simd ....
Time spent in matmul_simd_forward: 0.000085 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.005243
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000086 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.338005
Training accuracy: 0.000000

epoch: 17
matmul simd ....
Time spent in matmul_simd_forward: 0.000103 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = -0.043403
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.311851
Training accuracy: 0.062500

epoch: 18
matmul simd ....
Time spent in matmul_simd_forward: 0.000105 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = -0.043398
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000066 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.217372
Training accuracy: 0.312500

epoch: 19
matmul simd ....
Time spent in matmul_simd_forward: 0.000085 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005331
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.265325
Training accuracy: 0.000000

epoch: 20
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005522
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.380237
Training accuracy: 0.062500

epoch: 21
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.010566
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000104 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.296873
Training accuracy: 0.062500

epoch: 22
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = -0.040177
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000086 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.308312
Training accuracy: 0.125000

epoch: 23
matmul simd ....
Time spent in matmul_simd_forward: 0.000098 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.007545
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000093 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.265645
Training accuracy: 0.250000

epoch: 24
matmul simd ....
Time spent in matmul_simd_forward: 0.000143 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005389
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000120 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.344553
Training accuracy: 0.062500

epoch: 25
matmul simd ....
Time spent in matmul_simd_forward: 0.000130 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.005440
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000147 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.279312
Training accuracy: 0.062500

epoch: 26
matmul simd ....
Time spent in matmul_simd_forward: 0.000091 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000006 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.008498
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000068 seconds
Layer 0 weight grad [0][0] = 0.003165


Training loss: 2.290102
Training accuracy: 0.187500

epoch: 27
matmul simd ....
Time spent in matmul_simd_forward: 0.000128 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.006435
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000123 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.285518
Training accuracy: 0.062500

epoch: 28
matmul simd ....
Time spent in matmul_simd_forward: 0.000086 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005451
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000065 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.488722
Training accuracy: 0.000000

epoch: 29
matmul simd ....
Time spent in matmul_simd_forward: 0.000151 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005357
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000123 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.310664
Training accuracy: 0.000000

epoch: 30
matmul simd ....
Time spent in matmul_simd_forward: 0.000117 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000006 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.009982
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000091 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.324634
Training accuracy: 0.125000

epoch: 31
matmul simd ....
Time spent in matmul_simd_forward: 0.000122 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.010309
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.004447


Training loss: 2.397753
Training accuracy: 0.062500

epoch: 32
matmul simd ....
Time spent in matmul_simd_forward: 0.000083 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005210
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000107 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.434411
Training accuracy: 0.062500

epoch: 33
matmul simd ....
Time spent in matmul_simd_forward: 0.000089 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000006 seconds
Layer 1 weight grad [0][0] = 0.005229
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000070 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.310157
Training accuracy: 0.125000

epoch: 34
matmul simd ....
Time spent in matmul_simd_forward: 0.000086 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005051
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000133 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.376580
Training accuracy: 0.062500

epoch: 35
matmul simd ....
Time spent in matmul_simd_forward: 0.000101 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.010212
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.270572
Training accuracy: 0.125000

epoch: 36
matmul simd ....
Time spent in matmul_simd_forward: 0.000123 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.005155
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000123 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.384622
Training accuracy: 0.000000

epoch: 37
matmul simd ....
Time spent in matmul_simd_forward: 0.000145 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.006281
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000112 seconds
Layer 0 weight grad [0][0] = -0.001291


Training loss: 2.355891
Training accuracy: 0.000000

epoch: 38
matmul simd ....
Time spent in matmul_simd_forward: 0.000085 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005374
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.227645
Training accuracy: 0.250000

epoch: 39
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.009216
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = -0.006081


Training loss: 2.180305
Training accuracy: 0.312500

epoch: 40
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000003 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.005377
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.258875
Training accuracy: 0.250000

epoch: 41
matmul simd ....
Time spent in matmul_simd_forward: 0.000083 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005100
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.305506
Training accuracy: 0.000000

epoch: 42
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000003 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = -0.043671
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.342023
Training accuracy: 0.125000

epoch: 43
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.005287
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.283873
Training accuracy: 0.000000

epoch: 44
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000003 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005034
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000065 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.318188
Training accuracy: 0.062500

epoch: 45
matmul simd ....
Time spent in matmul_simd_forward: 0.000120 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005238
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.385760
Training accuracy: 0.125000

epoch: 46
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.011288
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000065 seconds
Layer 0 weight grad [0][0] = -0.006952


Training loss: 2.312948
Training accuracy: 0.187500

epoch: 47
matmul simd ....
Time spent in matmul_simd_forward: 0.000085 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000003 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005352
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000092 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.364224
Training accuracy: 0.062500

epoch: 48
matmul simd ....
Time spent in matmul_simd_forward: 0.000130 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.005084
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000108 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.296008
Training accuracy: 0.062500

epoch: 49
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005206
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000086 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.370130
Training accuracy: 0.062500

epoch: 50
matmul simd ....
Time spent in matmul_simd_forward: 0.000132 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005144
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000091 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.401769
Training accuracy: 0.062500

epoch: 51
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005050
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.303543
Training accuracy: 0.125000

epoch: 52
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.007197
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.243659
Training accuracy: 0.125000

epoch: 53
matmul simd ....
Time spent in matmul_simd_forward: 0.000092 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000009 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = -0.024654
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000085 seconds
Layer 0 weight grad [0][0] = 0.009949


Training loss: 2.334457
Training accuracy: 0.000000

epoch: 54
matmul simd ....
Time spent in matmul_simd_forward: 0.000119 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005453
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.320595
Training accuracy: 0.000000

epoch: 55
matmul simd ....
Time spent in matmul_simd_forward: 0.000083 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005247
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000066 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.323905
Training accuracy: 0.125000

epoch: 56
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004994
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.349038
Training accuracy: 0.062500

epoch: 57
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005142
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.329905
Training accuracy: 0.000000

epoch: 58
matmul simd ....
Time spent in matmul_simd_forward: 0.000083 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000003 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005047
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000065 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.347441
Training accuracy: 0.125000

epoch: 59
matmul simd ....
Time spent in matmul_simd_forward: 0.000083 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.009653
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.318032
Training accuracy: 0.000000

epoch: 60
matmul simd ....
Time spent in matmul_simd_forward: 0.000148 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.009146
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = -0.018864


Training loss: 2.297318
Training accuracy: 0.125000

epoch: 61
matmul simd ....
Time spent in matmul_simd_forward: 0.000083 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000003 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004830
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000091 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.342341
Training accuracy: 0.125000

epoch: 62
matmul simd ....
Time spent in matmul_simd_forward: 0.000137 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.005113
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000131 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.367800
Training accuracy: 0.125000

epoch: 63
matmul simd ....
Time spent in matmul_simd_forward: 0.000149 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.004920
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000119 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.363548
Training accuracy: 0.062500

epoch: 64
matmul simd ....
Time spent in matmul_simd_forward: 0.000205 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000124 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = -0.043633
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000244 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.312300
Training accuracy: 0.062500

epoch: 65
matmul simd ....
Time spent in matmul_simd_forward: 0.000109 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.005124
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000122 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.325305
Training accuracy: 0.125000

epoch: 66
matmul simd ....
Time spent in matmul_simd_forward: 0.000097 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004923
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.377930
Training accuracy: 0.000000

epoch: 67
matmul simd ....
Time spent in matmul_simd_forward: 0.000088 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.005071
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000103 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.274700
Training accuracy: 0.062500

epoch: 68
matmul simd ....
Time spent in matmul_simd_forward: 0.000085 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005025
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.274615
Training accuracy: 0.125000

epoch: 69
matmul simd ....
Time spent in matmul_simd_forward: 0.000085 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.005015
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000065 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.312625
Training accuracy: 0.125000

epoch: 70
matmul simd ....
Time spent in matmul_simd_forward: 0.000083 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005074
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.284119
Training accuracy: 0.125000

epoch: 71
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000003 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.004953
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.310042
Training accuracy: 0.125000

epoch: 72
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.009573
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000065 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.317501
Training accuracy: 0.000000

epoch: 73
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = -0.043804
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.313750
Training accuracy: 0.062500

epoch: 74
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004932
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.289930
Training accuracy: 0.125000

epoch: 75
matmul simd ....
Time spent in matmul_simd_forward: 0.000083 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000003 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.008323
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.367134
Training accuracy: 0.062500

epoch: 76
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004879
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.335223
Training accuracy: 0.062500

epoch: 77
matmul simd ....
Time spent in matmul_simd_forward: 0.000103 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.007968
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000105 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.452292
Training accuracy: 0.000000

epoch: 78
matmul simd ....
Time spent in matmul_simd_forward: 0.000100 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004976
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000072 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.335825
Training accuracy: 0.125000

epoch: 79
matmul simd ....
Time spent in matmul_simd_forward: 0.000109 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000006 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004846
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.315064
Training accuracy: 0.125000

epoch: 80
matmul simd ....
Time spent in matmul_simd_forward: 0.000115 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000006 seconds
Layer 1 weight grad [0][0] = 0.005019
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000066 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.354992
Training accuracy: 0.125000

epoch: 81
matmul simd ....
Time spent in matmul_simd_forward: 0.000123 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.006840
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000085 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.300711
Training accuracy: 0.062500

epoch: 82
matmul simd ....
Time spent in matmul_simd_forward: 0.000151 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.009556
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000087 seconds
Layer 0 weight grad [0][0] = 0.004694


Training loss: 2.322652
Training accuracy: 0.062500

epoch: 83
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.008752
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.343363
Training accuracy: 0.000000

epoch: 84
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004755
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000091 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.314765
Training accuracy: 0.125000

epoch: 85
matmul simd ....
Time spent in matmul_simd_forward: 0.000085 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.004587
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000079 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.366608
Training accuracy: 0.062500

epoch: 86
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004578
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.294923
Training accuracy: 0.125000

epoch: 87
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000003 seconds
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.015767
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000090 seconds
Layer 0 weight grad [0][0] = -0.019361


Training loss: 2.289533
Training accuracy: 0.187500

epoch: 88
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000003 seconds
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004818
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.314389
Training accuracy: 0.062500

epoch: 89
matmul simd ....
Time spent in matmul_simd_forward: 0.000101 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004981
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.329076
Training accuracy: 0.000000

epoch: 90
matmul simd ....
Time spent in matmul_simd_forward: 0.000105 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.008715
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.313707
Training accuracy: 0.000000

epoch: 91
matmul simd ....
Time spent in matmul_simd_forward: 0.000085 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000003 seconds
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = -0.043782
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.256179
Training accuracy: 0.062500

epoch: 92
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.006687
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.005711


Training loss: 2.348064
Training accuracy: 0.062500

epoch: 93
matmul simd ....
Time spent in matmul_simd_forward: 0.000083 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000003 seconds
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.004934
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000091 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.345662
Training accuracy: 0.062500

epoch: 94
matmul simd ....
Time spent in matmul_simd_forward: 0.000085 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.004952
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.274757
Training accuracy: 0.062500

epoch: 95
matmul simd ....
Time spent in matmul_simd_forward: 0.000083 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000003 seconds
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = -0.039626
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000078 seconds
Layer 0 weight grad [0][0] = -0.024577


Training loss: 2.281611
Training accuracy: 0.125000

epoch: 96
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.009672
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.318667
Training accuracy: 0.125000

epoch: 97
matmul simd ....
Time spent in matmul_simd_forward: 0.000083 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000003 seconds
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.005254
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.337492
Training accuracy: 0.000000

epoch: 98
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.006987
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.302535
Training accuracy: 0.062500

epoch: 99
matmul simd ....
Time spent in matmul_simd_forward: 0.000084 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.005062
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.323681
Training accuracy: 0.062500

saving in file: /home/rickojn/coding/cf-mlp/models/model_20260119_221750_h8.mdl
Error opening file /home/rickojn/coding/cf-mlp/models/matmul simd ....
Time spent in matmul_simd_forward: 0.119610 seconds
relu forward ...
Time spent in relu_forward: 0.000183 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.002035 seconds
softmax forward ...
Time spent in softmax_forward: 0.000608 seconds
Test loss after training: 2.332086
Test accuracy after training: 0.100300
