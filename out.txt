rows: 28, cols: 28
Number of training images: 60000
rows: 28, cols: 28
Number of test images: 10000
model layers = 0
Model created with 2 layers
Layer 0: 784 inputs, 8 neurons
Layer 1: 8 inputs, 10 neurons
Number of parameters: 6370
Batch size: 16
Error opening directory /home/rickojn/coding/cf-mlp/models/No model found, training from scratch
relu forward ...
Time spent in relu_forward: 0.000147 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000680 seconds
Test loss before training: 2.322339
Test accuracy before training: 0.099200


training loop:

epoch: 0
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.009273
relu backward ...
Time spent in relu_backward: 0.000000 seconds
matmul backward ...
Time spent in matmul_backward: 0.000058 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.319504
Training accuracy: 0.125000

epoch: 1
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004433
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000058 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.306472
Training accuracy: 0.312500

epoch: 2
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.008667
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000061 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.311563
Training accuracy: 0.125000

epoch: 3
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005851
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000065 seconds
Layer 0 weight grad [0][0] = 0.002156


Training loss: 2.303802
Training accuracy: 0.187500

epoch: 4
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.014218
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000058 seconds
Layer 0 weight grad [0][0] = 0.035293


Training loss: 2.268185
Training accuracy: 0.062500

epoch: 5
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.006031
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000061 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.280582
Training accuracy: 0.125000

epoch: 6
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.004429
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000097 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.352071
Training accuracy: 0.062500

epoch: 7
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004435
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000065 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.350573
Training accuracy: 0.000000

epoch: 8
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.009270
relu backward ...
Time spent in relu_backward: 0.000000 seconds
matmul backward ...
Time spent in matmul_backward: 0.000067 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.303458
Training accuracy: 0.125000

epoch: 9
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = -0.044344
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.352024
Training accuracy: 0.125000

epoch: 10
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004442
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000087 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.241038
Training accuracy: 0.062500

epoch: 11
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000006 seconds
Layer 1 weight grad [0][0] = 0.004450
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000080 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.332704
Training accuracy: 0.000000

epoch: 12
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004619
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000113 seconds
Layer 0 weight grad [0][0] = 0.000785


Training loss: 2.307041
Training accuracy: 0.062500

epoch: 13
relu forward ...
Time spent in relu_forward: 0.000003 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.008985
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000121 seconds
Layer 0 weight grad [0][0] = -0.013179


Training loss: 2.285651
Training accuracy: 0.250000

epoch: 14
relu forward ...
Time spent in relu_forward: 0.000004 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000003 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.005656
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000089 seconds
Layer 0 weight grad [0][0] = 0.000275


Training loss: 2.335412
Training accuracy: 0.125000

epoch: 15
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.004434
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000062 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.276649
Training accuracy: 0.125000

epoch: 16
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004428
relu backward ...
Time spent in relu_backward: 0.000004 seconds
matmul backward ...
Time spent in matmul_backward: 0.000066 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.318434
Training accuracy: 0.187500

epoch: 17
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = -0.044348
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.329567
Training accuracy: 0.000000

epoch: 18
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = -0.044341
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000066 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.328455
Training accuracy: 0.125000

epoch: 19
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004440
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000716 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.282414
Training accuracy: 0.125000

epoch: 20
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004440
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000091 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.317503
Training accuracy: 0.125000

epoch: 21
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.009271
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.267194
Training accuracy: 0.125000

epoch: 22
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = -0.041336
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000062 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.304780
Training accuracy: 0.062500

epoch: 23
relu forward ...
Time spent in relu_forward: 0.000005 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.007186
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.286090
Training accuracy: 0.062500

epoch: 24
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000007 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.004445
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000067 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.258586
Training accuracy: 0.125000

epoch: 25
relu forward ...
Time spent in relu_forward: 0.000003 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.004446
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000119 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.280102
Training accuracy: 0.000000

epoch: 26
relu forward ...
Time spent in relu_forward: 0.000004 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.007910
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000129 seconds
Layer 0 weight grad [0][0] = 0.002408


Training loss: 2.331263
Training accuracy: 0.062500

epoch: 27
relu forward ...
Time spent in relu_forward: 0.000003 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.005189
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000092 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.249856
Training accuracy: 0.187500

epoch: 28
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004447
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000062 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.243092
Training accuracy: 0.187500

epoch: 29
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.004447
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.293853
Training accuracy: 0.000000

epoch: 30
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.009166
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.285823
Training accuracy: 0.125000

epoch: 31
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.006781
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.006280


Training loss: 2.383260
Training accuracy: 0.187500

epoch: 32
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004448
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000066 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.265952
Training accuracy: 0.125000

epoch: 33
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004444
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.319443
Training accuracy: 0.062500

epoch: 34
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004433
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.364524
Training accuracy: 0.062500

epoch: 35
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.007107
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.280892
Training accuracy: 0.062500

epoch: 36
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004423
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.333827
Training accuracy: 0.062500

epoch: 37
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005489
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000067 seconds
Layer 0 weight grad [0][0] = -0.001274


Training loss: 2.287858
Training accuracy: 0.250000

epoch: 38
relu forward ...
Time spent in relu_forward: 0.000003 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000006 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000003 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.004436
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000094 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.351898
Training accuracy: 0.000000

epoch: 39
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.006965
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000108 seconds
Layer 0 weight grad [0][0] = -0.008564


Training loss: 2.292016
Training accuracy: 0.062500

epoch: 40
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.004425
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000080 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.376498
Training accuracy: 0.062500

epoch: 41
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000006 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.004422
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000093 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.287949
Training accuracy: 0.125000

epoch: 42
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = -0.044355
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.330471
Training accuracy: 0.125000

epoch: 43
relu forward ...
Time spent in relu_forward: 0.000003 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004430
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000086 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.329664
Training accuracy: 0.000000

epoch: 44
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004435
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000067 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.310668
Training accuracy: 0.125000

epoch: 45
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004434
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000111 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.280212
Training accuracy: 0.125000

epoch: 46
relu forward ...
Time spent in relu_forward: 0.000003 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000006 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.009657
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000090 seconds
Layer 0 weight grad [0][0] = -0.007763


Training loss: 2.348230
Training accuracy: 0.250000

epoch: 47
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.004431
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000087 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.273521
Training accuracy: 0.000000

epoch: 48
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004428
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000068 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.307910
Training accuracy: 0.062500

epoch: 49
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004425
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000064 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.317055
Training accuracy: 0.125000

epoch: 50
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004415
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000068 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.349847
Training accuracy: 0.000000

epoch: 51
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004413
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000065 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.349189
Training accuracy: 0.000000

epoch: 52
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.006265
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000118 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.268614
Training accuracy: 0.125000

epoch: 53
relu forward ...
Time spent in relu_forward: 0.000005 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = -0.025580
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000125 seconds
Layer 0 weight grad [0][0] = 0.007049


Training loss: 2.288011
Training accuracy: 0.000000

epoch: 54
relu forward ...
Time spent in relu_forward: 0.000004 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.004412
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000090 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.342138
Training accuracy: 0.125000

epoch: 55
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000006 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.004402
relu backward ...
Time spent in relu_backward: 0.000003 seconds
matmul backward ...
Time spent in matmul_backward: 0.000107 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.325607
Training accuracy: 0.000000

epoch: 56
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004399
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.295723
Training accuracy: 0.000000

epoch: 57
relu forward ...
Time spent in relu_forward: 0.000003 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004398
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000109 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.291193
Training accuracy: 0.187500

epoch: 58
relu forward ...
Time spent in relu_forward: 0.000025 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.004398
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000067 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.374061
Training accuracy: 0.125000

epoch: 59
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.008681
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.314168
Training accuracy: 0.000000

epoch: 60
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.008883
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000092 seconds
Layer 0 weight grad [0][0] = -0.020880


Training loss: 2.340314
Training accuracy: 0.125000

epoch: 61
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.004396
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.396951
Training accuracy: 0.250000

epoch: 62
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004410
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000095 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.304897
Training accuracy: 0.062500

epoch: 63
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004410
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.273007
Training accuracy: 0.000000

epoch: 64
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = -0.044364
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000062 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.285949
Training accuracy: 0.062500

epoch: 65
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.004429
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000062 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.285753
Training accuracy: 0.187500

epoch: 66
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000006 seconds
Layer 1 weight grad [0][0] = 0.004427
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000062 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.283896
Training accuracy: 0.062500

epoch: 67
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.004574
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000073 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.271899
Training accuracy: 0.125000

epoch: 68
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004427
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000085 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.376352
Training accuracy: 0.000000

epoch: 69
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000006 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.004426
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000083 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.327006
Training accuracy: 0.000000

epoch: 70
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004421
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000110 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.312794
Training accuracy: 0.000000

epoch: 71
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.004414
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000090 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.274498
Training accuracy: 0.000000

epoch: 72
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.009195
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000065 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.293230
Training accuracy: 0.062500

epoch: 73
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = -0.044366
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000087 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.368768
Training accuracy: 0.312500

epoch: 74
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.004431
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000094 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.298729
Training accuracy: 0.125000

epoch: 75
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.007865
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.349020
Training accuracy: 0.062500

epoch: 76
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004428
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000092 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.395769
Training accuracy: 0.062500

epoch: 77
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.008123
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000065 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.283499
Training accuracy: 0.125000

epoch: 78
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004419
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000083 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.285239
Training accuracy: 0.000000

epoch: 79
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004419
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000081 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.303679
Training accuracy: 0.062500

epoch: 80
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.004411
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000062 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.321757
Training accuracy: 0.062500

epoch: 81
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.006459
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.352191
Training accuracy: 0.062500

epoch: 82
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.010266
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000066 seconds
Layer 0 weight grad [0][0] = 0.001788


Training loss: 2.273670
Training accuracy: 0.062500

epoch: 83
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.007649
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000138 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.384674
Training accuracy: 0.000000

epoch: 84
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.004440
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000102 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.348893
Training accuracy: 0.062500

epoch: 85
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004436
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000090 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.311948
Training accuracy: 0.125000

epoch: 86
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004433
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000062 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.316201
Training accuracy: 0.000000

epoch: 87
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.013133
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000062 seconds
Layer 0 weight grad [0][0] = -0.026521


Training loss: 2.302982
Training accuracy: 0.312500

epoch: 88
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004421
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.286151
Training accuracy: 0.187500

epoch: 89
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004417
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000065 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.276915
Training accuracy: 0.125000

epoch: 90
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.007547
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000066 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.247765
Training accuracy: 0.250000

epoch: 91
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = -0.044357
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000094 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.335387
Training accuracy: 0.125000

epoch: 92
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005583
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000065 seconds
Layer 0 weight grad [0][0] = 0.005526


Training loss: 2.332080
Training accuracy: 0.125000

epoch: 93
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.004423
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000066 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.333511
Training accuracy: 0.125000

epoch: 94
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.004423
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.347825
Training accuracy: 0.125000

epoch: 95
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = -0.040174
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000116 seconds
Layer 0 weight grad [0][0] = -0.025520


Training loss: 2.327396
Training accuracy: 0.125000

epoch: 96
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.007092
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.280957
Training accuracy: 0.125000

epoch: 97
relu forward ...
Time spent in relu_forward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000002 seconds
Layer 1 weight grad [0][0] = 0.004430
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.329043
Training accuracy: 0.000000

epoch: 98
relu forward ...
Time spent in relu_forward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.006323
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000083 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.292564
Training accuracy: 0.125000

epoch: 99
relu forward ...
Time spent in relu_forward: 0.000003 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004412
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000063 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.305476
Training accuracy: 0.250000

saving in file: /home/rickojn/coding/cf-mlp/models/model_20260120_224820_h8.mdl
Error opening file /home/rickojn/coding/cf-mlp/models/relu forward ...
Time spent in relu_forward: 0.000160 seconds
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000612 seconds
Test loss after training: 2.323092
Test accuracy after training: 0.098600
