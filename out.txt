rows: 28, cols: 28
Number of training images: 60000
rows: 28, cols: 28
Number of test images: 10000
model layers = 0
Model created with 2 layers
Layer 0: 784 inputs, 8 neurons
Layer 1: 8 inputs, 10 neurons
Number of parameters: 6370
Batch size: 16

Loading model from file /home/patrick/coding/cf-mlp/models//model_20260117_224342_h8.mdl

Model loaded successfully
matmul simd ....
Time spent in matmul_simd_forward: 0.138467 seconds
relu forward ...
Time spent in relu_forward: 0.000797 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.002464 seconds
softmax forward ...
Time spent in softmax_forward: 0.000750 seconds
Test loss before training: 2.341712
Test accuracy before training: 0.098800


training loop:

epoch: 0
matmul simd ....
Time spent in matmul_simd_forward: 0.000124 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.005345
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000093 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.284177
Training accuracy: 0.062500

epoch: 1
matmul simd ....
Time spent in matmul_simd_forward: 0.000120 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000097 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.339771
Training accuracy: 0.125000

epoch: 2
matmul simd ....
Time spent in matmul_simd_forward: 0.000118 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000006 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.005039
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000093 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.355317
Training accuracy: 0.187500

epoch: 3
matmul simd ....
Time spent in matmul_simd_forward: 0.000118 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.002595
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000093 seconds
Layer 0 weight grad [0][0] = -0.011399


Training loss: 2.313375
Training accuracy: 0.062500

epoch: 4
matmul simd ....
Time spent in matmul_simd_forward: 0.000122 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000006 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.010907
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000094 seconds
Layer 0 weight grad [0][0] = 0.047120


Training loss: 2.351583
Training accuracy: 0.062500

epoch: 5
matmul simd ....
Time spent in matmul_simd_forward: 0.000136 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.002259
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000092 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.358512
Training accuracy: 0.062500

epoch: 6
matmul simd ....
Time spent in matmul_simd_forward: 0.000137 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000088 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.358042
Training accuracy: 0.062500

epoch: 7
matmul simd ....
Time spent in matmul_simd_forward: 0.000113 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000088 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.271187
Training accuracy: 0.125000

epoch: 8
matmul simd ....
Time spent in matmul_simd_forward: 0.000113 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005399
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000088 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.244145
Training accuracy: 0.375000

epoch: 9
matmul simd ....
Time spent in matmul_simd_forward: 0.000124 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000104 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.308204
Training accuracy: 0.062500

epoch: 10
matmul simd ....
Time spent in matmul_simd_forward: 0.000109 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000087 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.293620
Training accuracy: 0.000000

epoch: 11
matmul simd ....
Time spent in matmul_simd_forward: 0.000109 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000085 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.430806
Training accuracy: 0.000000

epoch: 12
matmul simd ....
Time spent in matmul_simd_forward: 0.000109 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000340
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000085 seconds
Layer 0 weight grad [0][0] = 0.000848


Training loss: 2.346954
Training accuracy: 0.125000

epoch: 13
matmul simd ....
Time spent in matmul_simd_forward: 0.000116 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.003627
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000085 seconds
Layer 0 weight grad [0][0] = -0.008727


Training loss: 2.250287
Training accuracy: 0.312500

epoch: 14
matmul simd ....
Time spent in matmul_simd_forward: 0.000109 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000722
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000086 seconds
Layer 0 weight grad [0][0] = -0.002949


Training loss: 2.300173
Training accuracy: 0.062500

epoch: 15
matmul simd ....
Time spent in matmul_simd_forward: 0.000109 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000085 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.335365
Training accuracy: 0.062500

epoch: 16
matmul simd ....
Time spent in matmul_simd_forward: 0.000110 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000086 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.325716
Training accuracy: 0.062500

epoch: 17
matmul simd ....
Time spent in matmul_simd_forward: 0.000117 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000085 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.288447
Training accuracy: 0.187500

epoch: 18
matmul simd ....
Time spent in matmul_simd_forward: 0.000109 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000086 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.239820
Training accuracy: 0.250000

epoch: 19
matmul simd ....
Time spent in matmul_simd_forward: 0.000109 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000086 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.295030
Training accuracy: 0.187500

epoch: 20
matmul simd ....
Time spent in matmul_simd_forward: 0.000113 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000086 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.379692
Training accuracy: 0.000000

epoch: 21
matmul simd ....
Time spent in matmul_simd_forward: 0.000135 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005355
relu backward ...
Time spent in relu_backward: 0.000004 seconds
matmul backward ...
Time spent in matmul_backward: 0.000138 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.336228
Training accuracy: 0.062500

epoch: 22
matmul simd ....
Time spent in matmul_simd_forward: 0.000106 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.003070
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000084 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.345543
Training accuracy: 0.000000

epoch: 23
matmul simd ....
Time spent in matmul_simd_forward: 0.000107 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.002041
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000136 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.227449
Training accuracy: 0.250000

epoch: 24
matmul simd ....
Time spent in matmul_simd_forward: 0.000150 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000006 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000006 seconds
matmul backward ...
Time spent in matmul_backward: 0.000100 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.305405
Training accuracy: 0.062500

epoch: 25
matmul simd ....
Time spent in matmul_simd_forward: 0.000127 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000135 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.296405
Training accuracy: 0.062500

epoch: 26
matmul simd ....
Time spent in matmul_simd_forward: 0.000115 seconds
relu forward ...
Time spent in relu_forward: 0.000004 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000006 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.002698
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000092 seconds
Layer 0 weight grad [0][0] = -0.011589


Training loss: 2.225179
Training accuracy: 0.250000

epoch: 27
matmul simd ....
Time spent in matmul_simd_forward: 0.000135 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000006 seconds
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.001240
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000111 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.308788
Training accuracy: 0.125000

epoch: 28
matmul simd ....
Time spent in matmul_simd_forward: 0.000116 seconds
relu forward ...
Time spent in relu_forward: 0.000004 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000006 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000094 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.433095
Training accuracy: 0.000000

epoch: 29
matmul simd ....
Time spent in matmul_simd_forward: 0.000126 seconds
relu forward ...
Time spent in relu_forward: 0.000004 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000092 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.326026
Training accuracy: 0.000000

epoch: 30
matmul simd ....
Time spent in matmul_simd_forward: 0.000111 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000006 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004970
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000088 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.278953
Training accuracy: 0.062500

epoch: 31
matmul simd ....
Time spent in matmul_simd_forward: 0.000119 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.003597
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000084 seconds
Layer 0 weight grad [0][0] = -0.020161


Training loss: 2.330114
Training accuracy: 0.125000

epoch: 32
matmul simd ....
Time spent in matmul_simd_forward: 0.000106 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000086 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.352320
Training accuracy: 0.000000

epoch: 33
matmul simd ....
Time spent in matmul_simd_forward: 0.000108 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000083 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.354043
Training accuracy: 0.000000

epoch: 34
matmul simd ....
Time spent in matmul_simd_forward: 0.000108 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000085 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.330868
Training accuracy: 0.000000

epoch: 35
matmul simd ....
Time spent in matmul_simd_forward: 0.000117 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005628
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000084 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.277071
Training accuracy: 0.125000

epoch: 36
matmul simd ....
Time spent in matmul_simd_forward: 0.000127 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000088 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.366918
Training accuracy: 0.062500

epoch: 37
matmul simd ....
Time spent in matmul_simd_forward: 0.000110 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000986
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000087 seconds
Layer 0 weight grad [0][0] = -0.000096


Training loss: 2.345740
Training accuracy: 0.062500

epoch: 38
matmul simd ....
Time spent in matmul_simd_forward: 0.000110 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000087 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.224845
Training accuracy: 0.250000

epoch: 39
matmul simd ....
Time spent in matmul_simd_forward: 0.000120 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.002681
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000087 seconds
Layer 0 weight grad [0][0] = 0.004830


Training loss: 2.226622
Training accuracy: 0.250000

epoch: 40
matmul simd ....
Time spent in matmul_simd_forward: 0.000126 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000083 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.315505
Training accuracy: 0.187500

epoch: 41
matmul simd ....
Time spent in matmul_simd_forward: 0.000120 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000082 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.311002
Training accuracy: 0.062500

epoch: 42
matmul simd ....
Time spent in matmul_simd_forward: 0.000107 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000083 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.328010
Training accuracy: 0.062500

epoch: 43
matmul simd ....
Time spent in matmul_simd_forward: 0.000116 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000083 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.239094
Training accuracy: 0.250000

epoch: 44
matmul simd ....
Time spent in matmul_simd_forward: 0.000106 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000084 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.316867
Training accuracy: 0.062500

epoch: 45
matmul simd ....
Time spent in matmul_simd_forward: 0.000106 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000084 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.315725
Training accuracy: 0.062500

epoch: 46
matmul simd ....
Time spent in matmul_simd_forward: 0.000106 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.006415
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000083 seconds
Layer 0 weight grad [0][0] = -0.008087


Training loss: 2.296244
Training accuracy: 0.250000

epoch: 47
matmul simd ....
Time spent in matmul_simd_forward: 0.000106 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000011 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000083 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.375823
Training accuracy: 0.000000

epoch: 48
matmul simd ....
Time spent in matmul_simd_forward: 0.000118 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000079 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.332460
Training accuracy: 0.062500

epoch: 49
matmul simd ....
Time spent in matmul_simd_forward: 0.000102 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000080 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.335993
Training accuracy: 0.000000

epoch: 50
matmul simd ....
Time spent in matmul_simd_forward: 0.000103 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000080 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.313754
Training accuracy: 0.062500

epoch: 51
matmul simd ....
Time spent in matmul_simd_forward: 0.000104 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000090 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.309175
Training accuracy: 0.125000

epoch: 52
matmul simd ....
Time spent in matmul_simd_forward: 0.000103 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.001825
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000081 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.231360
Training accuracy: 0.250000

epoch: 53
matmul simd ....
Time spent in matmul_simd_forward: 0.000102 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = -0.030532
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000081 seconds
Layer 0 weight grad [0][0] = 0.021580


Training loss: 2.342353
Training accuracy: 0.125000

epoch: 54
matmul simd ....
Time spent in matmul_simd_forward: 0.000103 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000080 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.313389
Training accuracy: 0.000000

epoch: 55
matmul simd ....
Time spent in matmul_simd_forward: 0.000102 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000080 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.256710
Training accuracy: 0.125000

epoch: 56
matmul simd ....
Time spent in matmul_simd_forward: 0.000112 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000080 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.324161
Training accuracy: 0.062500

epoch: 57
matmul simd ....
Time spent in matmul_simd_forward: 0.000102 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000081 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.329835
Training accuracy: 0.000000

epoch: 58
matmul simd ....
Time spent in matmul_simd_forward: 0.000102 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000081 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.334449
Training accuracy: 0.125000

epoch: 59
matmul simd ....
Time spent in matmul_simd_forward: 0.000102 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004800
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000082 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.301730
Training accuracy: 0.250000

epoch: 60
matmul simd ....
Time spent in matmul_simd_forward: 0.000108 seconds
relu forward ...
Time spent in relu_forward: 0.000004 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.003381
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000080 seconds
Layer 0 weight grad [0][0] = -0.020038


Training loss: 2.303579
Training accuracy: 0.125000

epoch: 61
matmul simd ....
Time spent in matmul_simd_forward: 0.000114 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000078 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.361862
Training accuracy: 0.000000

epoch: 62
matmul simd ....
Time spent in matmul_simd_forward: 0.000101 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000079 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.281202
Training accuracy: 0.062500

epoch: 63
matmul simd ....
Time spent in matmul_simd_forward: 0.000099 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000079 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.304846
Training accuracy: 0.187500

epoch: 64
matmul simd ....
Time spent in matmul_simd_forward: 0.000100 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000084 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.308232
Training accuracy: 0.187500

epoch: 65
matmul simd ....
Time spent in matmul_simd_forward: 0.000100 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000079 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.321081
Training accuracy: 0.187500

epoch: 66
matmul simd ....
Time spent in matmul_simd_forward: 0.000101 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000081 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.312450
Training accuracy: 0.062500

epoch: 67
matmul simd ....
Time spent in matmul_simd_forward: 0.000100 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000143
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000079 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.251405
Training accuracy: 0.187500

epoch: 68
matmul simd ....
Time spent in matmul_simd_forward: 0.000101 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000080 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.295819
Training accuracy: 0.250000

epoch: 69
matmul simd ....
Time spent in matmul_simd_forward: 0.000105 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000078 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.325520
Training accuracy: 0.062500

epoch: 70
matmul simd ....
Time spent in matmul_simd_forward: 0.000100 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000080 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.333788
Training accuracy: 0.000000

epoch: 71
matmul simd ....
Time spent in matmul_simd_forward: 0.000099 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000081 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.291270
Training accuracy: 0.062500

epoch: 72
matmul simd ....
Time spent in matmul_simd_forward: 0.000100 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.005163
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000078 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.283531
Training accuracy: 0.187500

epoch: 73
matmul simd ....
Time spent in matmul_simd_forward: 0.000100 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000087 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.322108
Training accuracy: 0.062500

epoch: 74
matmul simd ....
Time spent in matmul_simd_forward: 0.000101 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000079 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.306933
Training accuracy: 0.125000

epoch: 75
matmul simd ....
Time spent in matmul_simd_forward: 0.000101 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.003624
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000079 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.315845
Training accuracy: 0.062500

epoch: 76
matmul simd ....
Time spent in matmul_simd_forward: 0.000101 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000083 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.360848
Training accuracy: 0.062500

epoch: 77
matmul simd ....
Time spent in matmul_simd_forward: 0.000115 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.003326
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000100 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.310425
Training accuracy: 0.000000

epoch: 78
matmul simd ....
Time spent in matmul_simd_forward: 0.000118 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000091 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.309522
Training accuracy: 0.062500

epoch: 79
matmul simd ....
Time spent in matmul_simd_forward: 0.000105 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000106 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.314691
Training accuracy: 0.062500

epoch: 80
matmul simd ....
Time spent in matmul_simd_forward: 0.000102 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000078 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.324770
Training accuracy: 0.062500

epoch: 81
matmul simd ....
Time spent in matmul_simd_forward: 0.000100 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.001913
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000093 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.353826
Training accuracy: 0.000000

epoch: 82
matmul simd ....
Time spent in matmul_simd_forward: 0.000100 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.003660
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000117 seconds
Layer 0 weight grad [0][0] = -0.021894


Training loss: 2.290827
Training accuracy: 0.125000

epoch: 83
matmul simd ....
Time spent in matmul_simd_forward: 0.000113 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.004557
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000142 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.371067
Training accuracy: 0.062500

epoch: 84
matmul simd ....
Time spent in matmul_simd_forward: 0.000122 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000006 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000087 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.310027
Training accuracy: 0.062500

epoch: 85
matmul simd ....
Time spent in matmul_simd_forward: 0.000111 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000078 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.312116
Training accuracy: 0.000000

epoch: 86
matmul simd ....
Time spent in matmul_simd_forward: 0.000120 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000108 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.255039
Training accuracy: 0.125000

epoch: 87
matmul simd ....
Time spent in matmul_simd_forward: 0.000108 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.008876
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000120 seconds
Layer 0 weight grad [0][0] = -0.047458


Training loss: 2.383563
Training accuracy: 0.125000

epoch: 88
matmul simd ....
Time spent in matmul_simd_forward: 0.000123 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000143 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.315519
Training accuracy: 0.000000

epoch: 89
matmul simd ....
Time spent in matmul_simd_forward: 0.000145 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000090 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.274118
Training accuracy: 0.125000

epoch: 90
matmul simd ....
Time spent in matmul_simd_forward: 0.000115 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000006 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004282
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000081 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.363156
Training accuracy: 0.000000

epoch: 91
matmul simd ....
Time spent in matmul_simd_forward: 0.000141 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000171 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.286903
Training accuracy: 0.187500

epoch: 92
matmul simd ....
Time spent in matmul_simd_forward: 0.000172 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.001780
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000145 seconds
Layer 0 weight grad [0][0] = 0.005190


Training loss: 2.318980
Training accuracy: 0.187500

epoch: 93
matmul simd ....
Time spent in matmul_simd_forward: 0.000144 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000006 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000146 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.324394
Training accuracy: 0.000000

epoch: 94
matmul simd ....
Time spent in matmul_simd_forward: 0.000152 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000139 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.288782
Training accuracy: 0.062500

epoch: 95
matmul simd ....
Time spent in matmul_simd_forward: 0.000143 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000009 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.003383
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000155 seconds
Layer 0 weight grad [0][0] = -0.032619


Training loss: 2.303152
Training accuracy: 0.187500

epoch: 96
matmul simd ....
Time spent in matmul_simd_forward: 0.000159 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.005202
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000164 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.267505
Training accuracy: 0.187500

epoch: 97
matmul simd ....
Time spent in matmul_simd_forward: 0.000160 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000170 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.310101
Training accuracy: 0.062500

epoch: 98
matmul simd ....
Time spent in matmul_simd_forward: 0.000195 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000006 seconds
Layer 1 weight grad [0][0] = 0.001778
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000180 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.318036
Training accuracy: 0.062500

epoch: 99
matmul simd ....
Time spent in matmul_simd_forward: 0.000311 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000012 seconds
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000006 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000185 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.300673
Training accuracy: 0.125000

saving in file: /home/patrick/coding/cf-mlp/models/model_20260117_224405_h8.mdl
matmul simd ....
Time spent in matmul_simd_forward: 0.135704 seconds
relu forward ...
Time spent in relu_forward: 0.000507 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.002778 seconds
softmax forward ...
Time spent in softmax_forward: 0.000711 seconds
Test loss after training: 2.342589
Test accuracy after training: 0.098300
