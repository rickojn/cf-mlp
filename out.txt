minimum of 5 and 10: 5
rows: 28, cols: 28
Number of training images: 60000
rows: 28, cols: 28
Number of test images: 10000
Model created with 2 layers
Layer 0: 784 inputs, 8 neurons
Layer 1: 8 inputs, 10 neurons
Number of parameters: 6370
Batch size: 16

Loading model from file /home/patrick/coding/cf-mlp/models//model_20260111_182033_h8.mdl

Model loaded successfully
matmul simd ....
Time spent in matmul_simd_forward: 0.126764 seconds
relu forward ...
Time spent in relu_forward: 0.000243 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.002232 seconds
softmax forward ...
Time spent in softmax_forward: 0.000668 seconds
Test loss before training: 2.331763
Test accuracy before training: 0.100400


training loop:

epoch: 0
matmul simd ....
Time spent in matmul_simd_forward: 0.000107 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000006 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.004728
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000112 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.304641
Training accuracy: 0.062500

epoch: 1
matmul simd ....
Time spent in matmul_simd_forward: 0.000108 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000113 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.334953
Training accuracy: 0.062500

epoch: 2
matmul simd ....
Time spent in matmul_simd_forward: 0.000105 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004478
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000113 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.326182
Training accuracy: 0.125000

epoch: 3
matmul simd ....
Time spent in matmul_simd_forward: 0.000101 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.002896
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000113 seconds
Layer 0 weight grad [0][0] = -0.001568


Training loss: 2.315842
Training accuracy: 0.125000

epoch: 4
matmul simd ....
Time spent in matmul_simd_forward: 0.000103 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.012393
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000175 seconds
Layer 0 weight grad [0][0] = 0.041404


Training loss: 2.337988
Training accuracy: 0.062500

epoch: 5
matmul simd ....
Time spent in matmul_simd_forward: 0.000139 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.002188
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000147 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.353478
Training accuracy: 0.062500

epoch: 6
matmul simd ....
Time spent in matmul_simd_forward: 0.000106 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000153 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.353736
Training accuracy: 0.062500

epoch: 7
matmul simd ....
Time spent in matmul_simd_forward: 0.000122 seconds
relu forward ...
Time spent in relu_forward: 0.000004 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000146 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.272643
Training accuracy: 0.062500

epoch: 8
matmul simd ....
Time spent in matmul_simd_forward: 0.000112 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004812
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000113 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.236384
Training accuracy: 0.375000

epoch: 9
matmul simd ....
Time spent in matmul_simd_forward: 0.000101 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000113 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.326078
Training accuracy: 0.062500

epoch: 10
matmul simd ....
Time spent in matmul_simd_forward: 0.000104 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000161 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.307240
Training accuracy: 0.000000

epoch: 11
matmul simd ....
Time spent in matmul_simd_forward: 0.000111 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000113 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.405687
Training accuracy: 0.000000

epoch: 12
matmul simd ....
Time spent in matmul_simd_forward: 0.000120 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000354
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000131 seconds
Layer 0 weight grad [0][0] = 0.000866


Training loss: 2.341743
Training accuracy: 0.062500

epoch: 13
matmul simd ....
Time spent in matmul_simd_forward: 0.000099 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.004372
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000143 seconds
Layer 0 weight grad [0][0] = -0.010695


Training loss: 2.269318
Training accuracy: 0.187500

epoch: 14
matmul simd ....
Time spent in matmul_simd_forward: 0.000114 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000006 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000778
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000147 seconds
Layer 0 weight grad [0][0] = -0.000138


Training loss: 2.297279
Training accuracy: 0.062500

epoch: 15
matmul simd ....
Time spent in matmul_simd_forward: 0.000110 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000109 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.322579
Training accuracy: 0.062500

epoch: 16
matmul simd ....
Time spent in matmul_simd_forward: 0.000107 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000109 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.322669
Training accuracy: 0.000000

epoch: 17
matmul simd ....
Time spent in matmul_simd_forward: 0.000098 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000109 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.305753
Training accuracy: 0.125000

epoch: 18
matmul simd ....
Time spent in matmul_simd_forward: 0.000109 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000109 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.241194
Training accuracy: 0.250000

epoch: 19
matmul simd ....
Time spent in matmul_simd_forward: 0.000100 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000150 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.294274
Training accuracy: 0.125000

epoch: 20
matmul simd ....
Time spent in matmul_simd_forward: 0.000098 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000128 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.372626
Training accuracy: 0.000000

epoch: 21
matmul simd ....
Time spent in matmul_simd_forward: 0.000097 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004785
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000140 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.314457
Training accuracy: 0.062500

epoch: 22
matmul simd ....
Time spent in matmul_simd_forward: 0.000112 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.002890
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000130 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.338103
Training accuracy: 0.062500

epoch: 23
matmul simd ....
Time spent in matmul_simd_forward: 0.000109 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000006 seconds
Layer 1 weight grad [0][0] = 0.001965
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000126 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.234653
Training accuracy: 0.312500

epoch: 24
matmul simd ....
Time spent in matmul_simd_forward: 0.000115 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000106 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.303792
Training accuracy: 0.125000

epoch: 25
matmul simd ....
Time spent in matmul_simd_forward: 0.000127 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000106 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.298176
Training accuracy: 0.062500

epoch: 26
matmul simd ....
Time spent in matmul_simd_forward: 0.000128 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.002999
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000118 seconds
Layer 0 weight grad [0][0] = -0.000766


Training loss: 2.243176
Training accuracy: 0.250000

epoch: 27
matmul simd ....
Time spent in matmul_simd_forward: 0.000130 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000006 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.001215
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000136 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.298081
Training accuracy: 0.125000

epoch: 28
matmul simd ....
Time spent in matmul_simd_forward: 0.000131 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000138 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.393044
Training accuracy: 0.000000

epoch: 29
matmul simd ....
Time spent in matmul_simd_forward: 0.000122 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000138 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.326157
Training accuracy: 0.000000

epoch: 30
matmul simd ....
Time spent in matmul_simd_forward: 0.000121 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.004450
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000106 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.277784
Training accuracy: 0.062500

epoch: 31
matmul simd ....
Time spent in matmul_simd_forward: 0.000119 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004665
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000139 seconds
Layer 0 weight grad [0][0] = -0.002332


Training loss: 2.366346
Training accuracy: 0.187500

epoch: 32
matmul simd ....
Time spent in matmul_simd_forward: 0.000125 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000138 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.354354
Training accuracy: 0.062500

epoch: 33
matmul simd ....
Time spent in matmul_simd_forward: 0.000128 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000105 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.351399
Training accuracy: 0.000000

epoch: 34
matmul simd ....
Time spent in matmul_simd_forward: 0.000121 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000138 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.344795
Training accuracy: 0.000000

epoch: 35
matmul simd ....
Time spent in matmul_simd_forward: 0.000130 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004919
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000138 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.263368
Training accuracy: 0.187500

epoch: 36
matmul simd ....
Time spent in matmul_simd_forward: 0.000118 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000139 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.351593
Training accuracy: 0.062500

epoch: 37
matmul simd ....
Time spent in matmul_simd_forward: 0.000121 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.001002
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000106 seconds
Layer 0 weight grad [0][0] = -0.000960


Training loss: 2.343736
Training accuracy: 0.062500

epoch: 38
matmul simd ....
Time spent in matmul_simd_forward: 0.000123 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000154 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.217953
Training accuracy: 0.250000

epoch: 39
matmul simd ....
Time spent in matmul_simd_forward: 0.000122 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.003779
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000106 seconds
Layer 0 weight grad [0][0] = -0.003623


Training loss: 2.208943
Training accuracy: 0.250000

epoch: 40
matmul simd ....
Time spent in matmul_simd_forward: 0.000120 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000138 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.307172
Training accuracy: 0.187500

epoch: 41
matmul simd ....
Time spent in matmul_simd_forward: 0.000123 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000105 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.316388
Training accuracy: 0.000000

epoch: 42
matmul simd ....
Time spent in matmul_simd_forward: 0.000127 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000138 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.326781
Training accuracy: 0.062500

epoch: 43
matmul simd ....
Time spent in matmul_simd_forward: 0.000120 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000194 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.248167
Training accuracy: 0.187500

epoch: 44
matmul simd ....
Time spent in matmul_simd_forward: 0.000132 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000162 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.306899
Training accuracy: 0.062500

epoch: 45
matmul simd ....
Time spent in matmul_simd_forward: 0.000160 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000006 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000141 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.313822
Training accuracy: 0.062500

epoch: 46
matmul simd ....
Time spent in matmul_simd_forward: 0.000133 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.006136
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000138 seconds
Layer 0 weight grad [0][0] = -0.007581


Training loss: 2.296807
Training accuracy: 0.187500

epoch: 47
matmul simd ....
Time spent in matmul_simd_forward: 0.000126 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000106 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.395934
Training accuracy: 0.000000

epoch: 48
matmul simd ....
Time spent in matmul_simd_forward: 0.000123 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000143 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.320923
Training accuracy: 0.062500

epoch: 49
matmul simd ....
Time spent in matmul_simd_forward: 0.000145 seconds
relu forward ...
Time spent in relu_forward: 0.000004 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000124 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.345772
Training accuracy: 0.000000

epoch: 50
matmul simd ....
Time spent in matmul_simd_forward: 0.000128 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000103 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.312361
Training accuracy: 0.062500

epoch: 51
matmul simd ....
Time spent in matmul_simd_forward: 0.000121 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000105 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.291975
Training accuracy: 0.187500

epoch: 52
matmul simd ....
Time spent in matmul_simd_forward: 0.000119 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.001749
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000134 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.236623
Training accuracy: 0.187500

epoch: 53
matmul simd ....
Time spent in matmul_simd_forward: 0.000130 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = -0.029946
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000130 seconds
Layer 0 weight grad [0][0] = 0.011683


Training loss: 2.319432
Training accuracy: 0.125000

epoch: 54
matmul simd ....
Time spent in matmul_simd_forward: 0.000129 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000140 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.292291
Training accuracy: 0.062500

epoch: 55
matmul simd ....
Time spent in matmul_simd_forward: 0.000128 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000133 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.270972
Training accuracy: 0.187500

epoch: 56
matmul simd ....
Time spent in matmul_simd_forward: 0.000121 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000103 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.320206
Training accuracy: 0.062500

epoch: 57
matmul simd ....
Time spent in matmul_simd_forward: 0.000115 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000102 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.326937
Training accuracy: 0.000000

epoch: 58
matmul simd ....
Time spent in matmul_simd_forward: 0.000127 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000103 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.307823
Training accuracy: 0.187500

epoch: 59
matmul simd ....
Time spent in matmul_simd_forward: 0.000123 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004350
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000121 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.299515
Training accuracy: 0.250000

epoch: 60
matmul simd ....
Time spent in matmul_simd_forward: 0.000147 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.003916
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000123 seconds
Layer 0 weight grad [0][0] = -0.019906


Training loss: 2.308140
Training accuracy: 0.187500

epoch: 61
matmul simd ....
Time spent in matmul_simd_forward: 0.000135 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000009 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000109 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.327300
Training accuracy: 0.000000

epoch: 62
matmul simd ....
Time spent in matmul_simd_forward: 0.000121 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000105 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.293057
Training accuracy: 0.062500

epoch: 63
matmul simd ....
Time spent in matmul_simd_forward: 0.000115 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000102 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.295820
Training accuracy: 0.187500

epoch: 64
matmul simd ....
Time spent in matmul_simd_forward: 0.000121 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000126 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.308155
Training accuracy: 0.125000

epoch: 65
matmul simd ....
Time spent in matmul_simd_forward: 0.000127 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000006 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000131 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.321419
Training accuracy: 0.187500

epoch: 66
matmul simd ....
Time spent in matmul_simd_forward: 0.000133 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000137 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.310801
Training accuracy: 0.062500

epoch: 67
matmul simd ....
Time spent in matmul_simd_forward: 0.000133 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000143
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000102 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.261667
Training accuracy: 0.187500

epoch: 68
matmul simd ....
Time spent in matmul_simd_forward: 0.000125 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000102 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.284911
Training accuracy: 0.250000

epoch: 69
matmul simd ....
Time spent in matmul_simd_forward: 0.000115 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000109 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.330262
Training accuracy: 0.062500

epoch: 70
matmul simd ....
Time spent in matmul_simd_forward: 0.000124 seconds
relu forward ...
Time spent in relu_forward: 0.000004 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000134 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.331980
Training accuracy: 0.000000

epoch: 71
matmul simd ....
Time spent in matmul_simd_forward: 0.000137 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000110 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.293792
Training accuracy: 0.062500

epoch: 72
matmul simd ....
Time spent in matmul_simd_forward: 0.000135 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.004603
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000134 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.275210
Training accuracy: 0.250000

epoch: 73
matmul simd ....
Time spent in matmul_simd_forward: 0.000128 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000122 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.321015
Training accuracy: 0.062500

epoch: 74
matmul simd ....
Time spent in matmul_simd_forward: 0.000120 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000102 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.309469
Training accuracy: 0.125000

epoch: 75
matmul simd ....
Time spent in matmul_simd_forward: 0.000123 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.003362
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000127 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.314744
Training accuracy: 0.062500

epoch: 76
matmul simd ....
Time spent in matmul_simd_forward: 0.000117 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000140 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.327596
Training accuracy: 0.062500

epoch: 77
matmul simd ....
Time spent in matmul_simd_forward: 0.000129 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.003107
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000153 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.340201
Training accuracy: 0.000000

epoch: 78
matmul simd ....
Time spent in matmul_simd_forward: 0.000134 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000118 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.308997
Training accuracy: 0.062500

epoch: 79
matmul simd ....
Time spent in matmul_simd_forward: 0.000116 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000102 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.315059
Training accuracy: 0.062500

epoch: 80
matmul simd ....
Time spent in matmul_simd_forward: 0.000121 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000102 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.325003
Training accuracy: 0.062500

epoch: 81
matmul simd ....
Time spent in matmul_simd_forward: 0.000122 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000006 seconds
Layer 1 weight grad [0][0] = 0.001826
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000110 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.348205
Training accuracy: 0.000000

epoch: 82
matmul simd ....
Time spent in matmul_simd_forward: 0.000130 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004489
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000140 seconds
Layer 0 weight grad [0][0] = -0.002343


Training loss: 2.287569
Training accuracy: 0.125000

epoch: 83
matmul simd ....
Time spent in matmul_simd_forward: 0.000130 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.004064
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000122 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.354897
Training accuracy: 0.000000

epoch: 84
matmul simd ....
Time spent in matmul_simd_forward: 0.000128 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000103 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.306949
Training accuracy: 0.125000

epoch: 85
matmul simd ....
Time spent in matmul_simd_forward: 0.000122 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000102 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.319327
Training accuracy: 0.000000

epoch: 86
matmul simd ....
Time spent in matmul_simd_forward: 0.000120 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000103 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.250745
Training accuracy: 0.125000

epoch: 87
matmul simd ....
Time spent in matmul_simd_forward: 0.000127 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.010421
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000128 seconds
Layer 0 weight grad [0][0] = -0.027310


Training loss: 2.336372
Training accuracy: 0.187500

epoch: 88
matmul simd ....
Time spent in matmul_simd_forward: 0.000132 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000114 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.315166
Training accuracy: 0.000000

epoch: 89
matmul simd ....
Time spent in matmul_simd_forward: 0.000133 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000128 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.270500
Training accuracy: 0.125000

epoch: 90
matmul simd ....
Time spent in matmul_simd_forward: 0.000132 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.003904
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000103 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.319864
Training accuracy: 0.000000

epoch: 91
matmul simd ....
Time spent in matmul_simd_forward: 0.000130 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000102 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.298375
Training accuracy: 0.187500

epoch: 92
matmul simd ....
Time spent in matmul_simd_forward: 0.000121 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.001907
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000102 seconds
Layer 0 weight grad [0][0] = 0.005237


Training loss: 2.307243
Training accuracy: 0.187500

epoch: 93
matmul simd ....
Time spent in matmul_simd_forward: 0.000117 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000125 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.325093
Training accuracy: 0.000000

epoch: 94
matmul simd ....
Time spent in matmul_simd_forward: 0.000138 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000141 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.288416
Training accuracy: 0.125000

epoch: 95
matmul simd ....
Time spent in matmul_simd_forward: 0.000131 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.003992
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000102 seconds
Layer 0 weight grad [0][0] = -0.026727


Training loss: 2.300642
Training accuracy: 0.187500

epoch: 96
matmul simd ....
Time spent in matmul_simd_forward: 0.000171 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.004578
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000141 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.301614
Training accuracy: 0.062500

epoch: 97
matmul simd ....
Time spent in matmul_simd_forward: 0.000119 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000103 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.307860
Training accuracy: 0.062500

epoch: 98
matmul simd ....
Time spent in matmul_simd_forward: 0.000120 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.001718
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000110 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.309336
Training accuracy: 0.062500

epoch: 99
matmul simd ....
Time spent in matmul_simd_forward: 0.000117 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000103 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.300213
Training accuracy: 0.125000

saving in file: /home/patrick/coding/cf-mlp/models/model_20260112_003447_h8.mdl
matmul simd ....
Time spent in matmul_simd_forward: 0.121417 seconds
relu forward ...
Time spent in relu_forward: 0.000235 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.001399 seconds
softmax forward ...
Time spent in softmax_forward: 0.000655 seconds
Test loss after training: 2.331839
Test accuracy after training: 0.099500
