rows: 28, cols: 28
Number of training images: 60000
rows: 28, cols: 28
Number of test images: 10000
model layers = 0
Model created with 2 layers
Layer 0: 784 inputs, 8 neurons
Layer 1: 8 inputs, 10 neurons
Number of parameters: 6370
Batch size: 16
Error opening directory /home/rickojn/coding/cf-mlp/models/No model found, training from scratch
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000504 seconds
Test loss before training: 2.314452
Test accuracy before training: 0.043400


training loop:

epoch: 0
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.591746
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 17.246683


Training loss: 2.294698
Training accuracy: 0.125000

epoch: 1
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.035744
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.628881


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 2
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.464732
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.927918


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 3
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.357338
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.782003


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 4
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.476778
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.604683


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 5
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.372148
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.996270


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 6
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.035252
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.174398


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 7
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.388083
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.357323


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 8
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.020271
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.911156


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 9
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.038741
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.515544


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 10
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.042221
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.015268


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 11
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.469506
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.029577


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 12
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000019 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.452515
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.548897


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 13
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.048766
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 15.786231


Training loss: 2.302585
Training accuracy: 0.312500

epoch: 14
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.051104
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.064932


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 15
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000007 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.463250
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.815734


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 16
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.600644
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.944007


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 17
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.088573
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.229445


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 18
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.081469
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.404407


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 19
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.048816
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.442937


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 20
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.046334
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.280321


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 21
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.022120
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.271311


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 22
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.079378
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.878646


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 23
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.157916
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.318813


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 24
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.034876
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.513167


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 25
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.071910
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.040145


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 26
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.442708
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.784077


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 27
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.028477
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.082270


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 28
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -1.005971
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.191159


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 29
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000012 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.488409
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.185506


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 30
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.024150
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.824172


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 31
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.026368
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 16.295540


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 32
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.434718
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.058954


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 33
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.007196
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 16.313524


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 34
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.066179
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.350840


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 35
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.033679
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.402825


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 36
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.192642
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.605427


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 37
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.575450
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.322599


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 38
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.027854
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.902662


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 39
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.011099
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.759363


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 40
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.069663
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.791424


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 41
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.023161
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.061954


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 42
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.064811
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.960367


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 43
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.005829
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.751787


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 44
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.078196
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.668339


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 45
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.583542
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.672911


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 46
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.568503
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.251228


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 47
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.023028
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.419368


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 48
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.294566
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.241972


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 49
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.000060
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.182056


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 50
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.022820
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.597174


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 51
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.084333
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.450980


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 52
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.042709
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.015299


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 53
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.077358
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.440746


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 54
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.039491
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.053558


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 55
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.020306
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.829582


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 56
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.049439
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.426025


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 57
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.595246
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.054403


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 58
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.329690
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.313459


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 59
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.049171
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.325455


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 60
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.032346
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.937586


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 61
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.053404
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.031893


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 62
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.037830
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.640249


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 63
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.455031
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.188343


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 64
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.062170
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.258818


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 65
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.141966
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.593708


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 66
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.091576
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.709111


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 67
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.431748
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.076393


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 68
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.367039
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.139194


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 69
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.042165
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.408470


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 70
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.043051
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.008974


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 71
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.392763
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.782025


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 72
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.042459
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.803398


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 73
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.165093
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.421940


Training loss: 2.302585
Training accuracy: 0.375000

epoch: 74
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.154431
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.163044


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 75
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.467326
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.105482


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 76
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.046602
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.305834


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 77
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.045390
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.296471


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 78
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.471727
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.541376


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 79
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.356132
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.394252


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 80
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.460420
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.257986


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 81
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.025290
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.606570


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 82
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.179573
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.138618


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 83
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.042510
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.997014


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 84
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.079364
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.651299


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 85
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.473575
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.073660


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 86
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.122359
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.250221


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 87
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.479752
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 16.304438


Training loss: 2.302585
Training accuracy: 0.312500

epoch: 88
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.032793
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.584317


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 89
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.452481
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.486078


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 90
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.588530
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.872718


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 91
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.040906
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.272450


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 92
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.476668
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.672459


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 93
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.023038
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.357801


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 94
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.220580
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.286674


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 95
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.033395
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.164805


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 96
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.034285
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.103270


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 97
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.483301
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.204759


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 98
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.032786
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.864047


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 99
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.031370
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.592508


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 100
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.465649
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.032730


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 101
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.039300
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.530325


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 102
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.076521
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.871199


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 103
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.019770
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.894083


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 104
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.164993
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.268082


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 105
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.037221
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.023664


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 106
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.075465
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.646194


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 107
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.482033
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.496593


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 108
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.479769
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.663327


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 109
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.466144
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.050879


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 110
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.075943
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.556435


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 111
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.135851
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.986939


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 112
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.062727
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.420409


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 113
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.061221
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.261900


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 114
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.386915
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.010681


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 115
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.670119
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.448957


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 116
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.008923
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.828651


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 117
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.348335
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.356976


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 118
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.006385
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.202364


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 119
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.214632
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.833060


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 120
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.044006
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.607441


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 121
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.555870
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.366284


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 122
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.490721
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.551847


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 123
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.393628
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.254404


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 124
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.560369
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.880488


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 125
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.012981
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.277006


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 126
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.015352
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.419654


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 127
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.017939
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.689610


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 128
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.480062
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.052608


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 129
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.059900
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.739570


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 130
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.497355
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 13.728300


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 131
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.499575
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.903943


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 132
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.699202
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.547447


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 133
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.560751
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.695239


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 134
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.485667
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.315051


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 135
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.484308
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.368755


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 136
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.019934
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.179453


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 137
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.057727
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.153101


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 138
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.001538
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.509338


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 139
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.182787
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.947331


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 140
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.019879
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.836491


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 141
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.379028
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.014717


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 142
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.022150
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.986512


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 143
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.477914
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.494489


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 144
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.021597
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.553550


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 145
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.020640
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.227030


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 146
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.477337
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.931173


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 147
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.115143
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.482262


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 148
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.064004
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.371923


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 149
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.174099
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.382530


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 150
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.470903
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.631022


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 151
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.482469
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.844581


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 152
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.138109
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.503069


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 153
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.006511
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.746345


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 154
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.024362
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.971591


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 155
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.026686
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.331537


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 156
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.579474
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.174700


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 157
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.137711
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.702036


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 158
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.024673
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.421983


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 159
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.026924
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.225850


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 160
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.027914
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.596230


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 161
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.469013
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.441278


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 162
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.034563
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.120380


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 163
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.183631
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.494553


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 164
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.585817
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.953906


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 165
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.476840
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.887978


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 166
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.040805
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.530848


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 167
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.473180
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.062346


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 168
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.475488
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.421724


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 169
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.038598
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.103354


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 170
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.036477
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.994249


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 171
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.461336
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.363085


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 172
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.024753
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.784940


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 173
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.453138
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.861223


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 174
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.052053
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.813503


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 175
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.090788
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.816299


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 176
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.419337
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.368596


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 177
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.048598
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.757330


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 178
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.330476
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.497177


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 179
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.052418
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.282009


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 180
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.236919
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.600417


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 181
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.043551
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.833551


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 182
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.164348
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.010532


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 183
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.038116
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.067787


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 184
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.040746
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.696217


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 185
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.039723
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.160026


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 186
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.526412
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.609801


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 187
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.012605
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.679705


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 188
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.033748
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.961526


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 189
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.540139
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.804974


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 190
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -1.137481
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.407056


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 191
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.029767
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.722513


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 192
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.028517
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.793261


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 193
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.175137
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.398384


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 194
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.488177
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.321250


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 195
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.682221
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.873521


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 196
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.022463
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.164261


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 197
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.029048
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.085846


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 198
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.615702
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.704571


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 199
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.473283
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.936107


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 200
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.013606
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.089077


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 201
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.383501
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.976454


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 202
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.056410
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.508348


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 203
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -1.246664
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.406984


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 204
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.715978
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.110211


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 205
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.170590
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.347406


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 206
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.500422
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.388762


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 207
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.002212
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.460219


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 208
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.686303
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.513403


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 209
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.205298
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.527747


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 210
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.567332
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.353259


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 211
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.479590
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.558548


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 212
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -1.231189
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.341179


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 213
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.480786
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.243155


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 214
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.481376
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.613029


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 215
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.622405
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.517616


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 216
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.017715
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.399469


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 217
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.016247
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.570690


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 218
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.022371
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.158981


Training loss: 2.302585
Training accuracy: 0.312500

epoch: 219
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.470799
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.142689


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 220
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.030341
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.508109


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 221
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.174522
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.766487


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 222
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.027585
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.887516


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 223
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.023125
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.845591


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 224
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.474023
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.270250


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 225
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.035171
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.035112


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 226
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.023934
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.835441


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 227
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.593291
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.511524


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 228
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.453438
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.568090


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 229
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000006 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.047908
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 15.904634


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 230
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.048966
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.644031


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 231
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.449297
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.196453


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 232
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.051957
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.015039


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 233
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.047313
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.859776


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 234
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.048156
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.294871


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 235
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.026218
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.193748


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 236
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.160879
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.614861


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 237
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.023327
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.824851


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 238
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.039695
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.397654


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 239
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.038188
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.427679


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 240
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.458892
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.118574


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 241
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.044411
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.993129


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 242
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.041004
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.270255


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 243
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.074025
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.797105


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 244
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.476173
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.005133


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 245
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.040898
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.862892


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 246
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.588013
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.128174


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 247
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.371492
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.443338


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 248
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.042837
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.637794


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 249
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.047747
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.562522


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 250
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.080448
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.910690


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 251
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.042591
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.218836


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 252
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.044602
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.280483


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 253
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.045341
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.375806


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 254
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.456342
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.750990


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 255
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.025887
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.409988


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 256
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.044584
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.398976


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 257
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.473143
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.997372


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 258
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.024945
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.559399


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 259
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.578498
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.147017


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 260
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.526326
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 15.400590


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 261
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.256606
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.135149


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 262
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.083615
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.753649


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 263
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.080643
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.602273


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 264
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.038112
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.224209


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 265
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.018870
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.368141


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 266
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.663881
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.891249


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 267
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.856293
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.838115


Training loss: 2.302585
Training accuracy: 0.375000

epoch: 268
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.042511
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.328677


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 269
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.199881
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.964590


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 270
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.589572
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.442972


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 271
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.041958
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.247120


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 272
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.025210
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.899157


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 273
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.451879
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.567369


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 274
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.261012
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.246344


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 275
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.602375
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.914789


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 276
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.253208
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.056231


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 277
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.035753
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.075666


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 278
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.057998
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.854204


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 279
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.241711
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.693710


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 280
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.255984
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.726623


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 281
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.820670
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.175570


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 282
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.091631
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.575440


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 283
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.033413
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.522566


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 284
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.030911
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.801677


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 285
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.052886
relu backward ...
Time spent in relu_backward: 0.000003 seconds
Layer 0 weight grad [0][0] = 15.307407


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 286
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.032713
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.504210


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 287
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.031237
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.705730


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 288
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.053763
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.504484


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 289
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.444425
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.457583


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 290
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.458336
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.861442


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 291
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.457460
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.352945


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 292
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.059843
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.874231


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 293
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.454309
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.623829


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 294
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.440719
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.534527


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 295
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.062976
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.790330


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 296
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.172837
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.728280


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 297
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.061015
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 15.634570


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 298
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.045433
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.279460


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 299
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.063265
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.724019


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 300
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.529396
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.959460


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 301
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.104458
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.024946


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 302
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.647121
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.110966


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 303
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.441556
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.443074


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 304
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.031388
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.449916


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 305
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.038972
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 15.451205


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 306
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.057590
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.638089


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 307
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.442840
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.274864


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 308
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000016 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.042832
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 14.984420


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 309
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.158347
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.569186


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 310
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.063819
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.565557


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 311
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.031816
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.539053


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 312
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.062593
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.053410


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 313
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.063569
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.443723


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 314
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.064533
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.494756


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 315
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.449746
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.558758


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 316
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.067399
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.818123


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 317
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.445591
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.296916


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 318
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.044928
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.694734


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 319
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.062942
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.162636


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 320
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.100520
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.465422


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 321
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.205674
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.267483


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 322
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.061893
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.008474


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 323
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.143028
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.239519


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 324
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.146691
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.617644


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 325
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.055440
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 14.905864


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 326
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.286582
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.636248


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 327
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.446016
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.404470


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 328
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.095857
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.273677


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 329
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.054660
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.853310


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 330
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.453595
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.814101


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 331
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.607266
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.981979


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 332
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.146071
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 14.761158


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 333
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.096967
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.213547


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 334
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.082465
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.573512


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 335
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.051805
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.132080


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 336
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.054210
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.412636


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 337
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.087924
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.946589


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 338
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.465062
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.205396


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 339
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.175929
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.519225


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 340
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.046428
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.091841


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 341
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.455021
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.546210


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 342
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.857949
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.125274


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 343
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.047366
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.380457


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 344
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.044009
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.494165


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 345
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.046064
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.460814


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 346
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.047102
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.236646


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 347
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.595318
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.368832


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 348
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.673166
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.930735


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 349
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.161540
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.971247


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 350
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.040815
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 15.442280


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 351
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.038742
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.560209


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 352
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.077166
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.766275


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 353
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.039549
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.336594


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 354
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.041228
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.732223


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 355
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.042356
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.860725


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 356
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.594094
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.042259


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 357
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.159087
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.670845


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 358
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.162473
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.381272


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 359
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.091250
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.155936


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 360
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.028603
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.793830


Training loss: 2.302585
Training accuracy: 0.312500

epoch: 361
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.029821
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.135788


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 362
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.088378
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.956320


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 363
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000006 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.031813
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.422354


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 364
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.667850
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.245139


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 365
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.049660
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.501862


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 366
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.088369
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.360723


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 367
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.097797
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.459751


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 368
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.052808
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.214394


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 369
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.032714
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.658905


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 370
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.604497
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.341651


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 371
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.057043
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.032521


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 372
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.055438
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.545120


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 373
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.038797
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.145521


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 374
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.056671
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.609948


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 375
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.057558
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.853435


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 376
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.094170
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.087559


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 377
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.648567
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.369486


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 378
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.144461
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.657975


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 379
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.441157
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.696949


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 380
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.062711
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.340505


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 381
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.730296
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.139782


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 382
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.064062
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.291030


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 383
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.062143
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.013848


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 384
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.454684
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.307692


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 385
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.060261
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.646365


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 386
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.147452
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.834589


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 387
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.187681
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.357529


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 388
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.463787
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.354814


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 389
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.173093
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.880725


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 390
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.162154
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.391448


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 391
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.273665
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.490421


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 392
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.153497
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.125395


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 393
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.033279
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.141636


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 394
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.036300
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.843235


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 395
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.000495
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.650614


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 396
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.033568
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.821112


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 397
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.031991
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.754164


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 398
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.027655
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.632282


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 399
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.033085
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.530614


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 400
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.014728
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.726166


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 401
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.171498
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.987168


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 402
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.031524
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.522846


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 403
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.031064
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.382459


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 404
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.081915
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.439382


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 405
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.588159
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.401802


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 406
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.282190
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.079554


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 407
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.056397
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.043600


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 408
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.037101
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.950531


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 409
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.321729
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.730397


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 410
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.040181
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.336744


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 411
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.023920
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.745030


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 412
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.046729
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.201615


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 413
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.046487
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.725423


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 414
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.028025
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.703210


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 415
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.050880
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.816608


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 416
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.445535
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.830226


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 417
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.057953
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.111658


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 418
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.228138
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.016962


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 419
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.059551
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.967256


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 420
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.098638
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.115725


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 421
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.121187
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.144701


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 422
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.097511
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.453806


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 423
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.150390
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.661896


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 424
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.345948
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.482750


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 425
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.379322
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.683786


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 426
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.042958
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.461052


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 427
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.026543
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.429045


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 428
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.454466
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.789932


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 429
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.671406
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.674880


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 430
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.090923
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.630006


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 431
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.099114
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.562708


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 432
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.076746
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.460119


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 433
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.482542
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.969025


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 434
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.465321
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.109560


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 435
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.129990
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.817768


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 436
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.067866
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.943937


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 437
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.070937
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.716036


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 438
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.020949
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.520499


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 439
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.053953
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.260327


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 440
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.039280
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 15.165678


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 441
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.463348
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.916599


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 442
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.593311
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.730968


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 443
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.081337
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 13.724024


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 444
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.085298
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.185610


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 445
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.674242
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.793654


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 446
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.043392
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.969250


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 447
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.046091
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.280722


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 448
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.030917
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.962726


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 449
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.052098
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 16.231606


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 450
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.449285
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.907624


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 451
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.848919
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.297411


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 452
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.305278
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.537203


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 453
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.052471
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.531259


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 454
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.051378
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.621976


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 455
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.050230
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.249985


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 456
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.052737
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.420622


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 457
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.463731
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.742835


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 458
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.272078
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.413631


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 459
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.362751
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.008890


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 460
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.039318
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.948683


Training loss: 2.302585
Training accuracy: 0.312500

epoch: 461
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.016211
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.371594


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 462
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.067296
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.529525


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 463
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.576176
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.946073


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 464
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.028524
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.477503


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 465
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.009406
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.153147


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 466
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.030493
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.060472


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 467
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.010279
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.528118


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 468
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.176724
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.213253


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 469
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.083071
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.497528


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 470
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.061146
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.213266


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 471
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.418125
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.234266


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 472
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.203757
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.836194


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 473
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.988439
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.610313


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 474
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.020318
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.189817


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 475
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.021326
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.135496


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 476
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.046686
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.538429


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 477
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.021689
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.009109


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 478
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.019436
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.243471


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 479
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.188540
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.051115


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 480
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.013915
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.090199


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 481
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.158132
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.798580


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 482
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.096029
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.304623


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 483
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.035497
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.125623


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 484
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.569410
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.387438


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 485
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.026695
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 15.880985


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 486
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.005256
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.764116


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 487
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.128013
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.606400


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 488
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.725476
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.581948


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 489
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.028258
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.654016


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 490
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.013646
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.204088


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 491
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.014532
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.830869


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 492
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.512435
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.973854


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 493
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.180558
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.537825


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 494
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.661188
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.326626


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 495
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.125408
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.647806


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 496
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.017982
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.123037


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 497
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.019443
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.398079


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 498
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.017260
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.347610


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 499
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.021176
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.053986


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 500
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.018602
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.368788


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 501
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.533795
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.354618


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 502
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.512546
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.636551


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 503
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.541884
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.838084


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 504
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.005319
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.120426


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 505
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.003850
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.119564


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 506
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.545813
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.751639


Training loss: 2.302585
Training accuracy: 0.375000

epoch: 507
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.001368
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.236180


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 508
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.210682
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.078527


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 509
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.008040
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.693295


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 510
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.074332
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.923688


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 511
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.029976
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.394478


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 512
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.509713
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 15.258013


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 513
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.022280
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.008337


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 514
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.002941
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.488485


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 515
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.004846
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.695031


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 516
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.225548
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.536503


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 517
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.020680
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.047701


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 518
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.637755
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.643171


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 519
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.553651
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.793525


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 520
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.203764
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.235870


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 521
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.531462
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 15.270087


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 522
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.004284
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.538682


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 523
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.041677
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.749721


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 524
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.003005
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.757876


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 525
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.514310
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.392280


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 526
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.002045
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.031645


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 527
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.537181
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.366281


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 528
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.195848
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.469583


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 529
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.006651
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.524481


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 530
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.005619
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.447140


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 531
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.496764
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.221750


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 532
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.004770
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.948513


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 533
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.011566
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 15.626489


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 534
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.007675
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.134468


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 535
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.080237
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.371732


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 536
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.008802
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.321541


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 537
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.487614
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.380112


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 538
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.071736
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.066605


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 539
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.489084
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.729737


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 540
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.566675
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.003653


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 541
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.019267
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.701473


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 542
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.572355
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.212858


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 543
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.024860
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.974430


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 544
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.963993
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.580865


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 545
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.470956
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.257450


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 546
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.032722
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 14.916135


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 547
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000006 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.012860
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.662229


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 548
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.053300
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 15.691748


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 549
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.471020
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.790089


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 550
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.483897
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.203272


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 551
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.481627
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.278425


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 552
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.586613
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.526525


Training loss: 2.302585
Training accuracy: 0.375000

epoch: 553
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.021318
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.147004


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 554
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.040248
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.779343


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 555
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.039867
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.168756


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 556
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.683389
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.695471


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 557
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.016299
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 15.483267


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 558
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.170451
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.411974


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 559
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.031925
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.784513


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 560
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.065917
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.745420


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 561
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.025080
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.656145


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 562
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.026419
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.255838


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 563
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.489198
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.475952


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 564
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.470585
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.399264


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 565
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.468288
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.985121


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 566
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.674472
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.935266


Training loss: 2.302585
Training accuracy: 0.312500

epoch: 567
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.030486
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.184541


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 568
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.015306
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.233679


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 569
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.074005
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.440351


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 570
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.072578
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.397419


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 571
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.015762
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.396875


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 572
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.585616
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.874445


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 573
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.461261
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.815711


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 574
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.458283
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.801412


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 575
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.041834
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.048235


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 576
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000034 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.021188
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.514635


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 577
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.043464
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.963226


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 578
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.596328
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.251200


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 579
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.635238
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.270407


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 580
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.158129
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.275116


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 581
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.455182
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.587106


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 582
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.453762
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 14.978143


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 583
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.047603
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.560332


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 584
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.027477
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 16.119741


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 585
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.044232
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.815656


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 586
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.124169
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.374246


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 587
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.037139
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.208714


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 588
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.254450
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.999684


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 589
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.124662
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.749161


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 590
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.480207
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.593941


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 591
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.034033
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 13.967570


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 592
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.672865
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.319983


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 593
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.137263
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.438150


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 594
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.025582
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.005665


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 595
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.676454
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.985751


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 596
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.028753
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.196283


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 597
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.174405
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.384550


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 598
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.470928
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.530762


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 599
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.466176
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.549774


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 600
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.174623
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.662226


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 601
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.210304
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.739212


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 602
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.027105
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 15.828230


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 603
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.231593
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.180557


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 604
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.011604
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.803753


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 605
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.030736
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.746698


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 606
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.723571
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.189006


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 607
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.035028
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.702796


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 608
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.586949
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.753701


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 609
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.475503
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.595829


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 610
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.130120
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.709403


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 611
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.134575
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.258270


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 612
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.162718
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.256960


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 613
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.044200
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.642356


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 614
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.041520
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.538051


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 615
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.021538
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.215339


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 616
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.038103
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.357939


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 617
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.474628
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.547935


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 618
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.163963
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.606508


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 619
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.038558
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.251218


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 620
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.037637
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 15.447437


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 621
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.134041
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.679464


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 622
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.028705
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.203377


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 623
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.467635
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.326961


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 624
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.584618
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.714832


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 625
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.461834
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.482822


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 626
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.078134
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.423187


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 627
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.039498
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.459845


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 628
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.041880
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.436840


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 629
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.095555
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 15.827162


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 630
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.050675
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.036314


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 631
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.049572
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.832236


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 632
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.117806
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.287320


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 633
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.045857
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.149918


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 634
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.040346
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.431558


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 635
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.699842
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.174778


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 636
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.197658
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.901302


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 637
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.071546
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.446659


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 638
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.034439
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.953804


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 639
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.153729
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.765730


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 640
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.259465
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.703087


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 641
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.034182
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.407103


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 642
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.035469
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.967674


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 643
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.071308
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.377596


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 644
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.173172
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.099989


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 645
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.469946
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.270982


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 646
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.484619
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.323125


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 647
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.065602
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.842604


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 648
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.028392
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.922925


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 649
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.013173
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.837872


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 650
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.480828
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.406153


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 651
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.478705
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.914433


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 652
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.587917
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.263429


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 653
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.474892
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.349312


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 654
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.461282
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.565585


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 655
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.025931
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 15.464602


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 656
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.470464
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.408526


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 657
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.160207
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.701367


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 658
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.041982
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.237717


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 659
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.454836
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.942562


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 660
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.048505
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.993098


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 661
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.069189
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 14.993168


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 662
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.048293
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.186845


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 663
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.078212
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.077089


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 664
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.058995
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.293676


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 665
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.041725
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.370878


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 666
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.274726
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.495188


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 667
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.045663
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.390920


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 668
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.047389
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.561867


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 669
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.120609
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.149475


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 670
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.164583
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.841060


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 671
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.037126
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.349216


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 672
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.134319
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.460665


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 673
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.471659
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.984640


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 674
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.031491
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.745241


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 675
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.029766
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.554371


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 676
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.389519
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.434118


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 677
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.171948
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.586696


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 678
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.029538
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.014467


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 679
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.011922
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.659417


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 680
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.034803
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.808305


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 681
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.037940
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.003381


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 682
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.035860
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.709823


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 683
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.185999
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.673069


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 684
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.584350
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.995687


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 685
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.035897
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.458397


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 686
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.465397
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.552907


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 687
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.666277
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.254169


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 688
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.013591
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.487334


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 689
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.018403
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.406238


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 690
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.463046
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.570940


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 691
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.040774
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.738111


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 692
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.038273
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.407885


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 693
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.074036
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.678642


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 694
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.037298
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.272824


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 695
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.038479
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.550600


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 696
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.077263
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.358458


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 697
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.567452
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.275468


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 698
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.365381
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.742951


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 699
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.042538
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.851980


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 700
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.633093
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.963478


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 701
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.354896
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.964857


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 702
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.467296
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.488503


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 703
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.158297
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.531347


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 704
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.456617
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.531249


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 705
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.045482
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.173698


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 706
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.041758
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 14.914986


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 707
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.042582
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.832175


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 708
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.076062
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.192387


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 709
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.037186
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.166964


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 710
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.017679
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.988381


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 711
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.465628
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.607620


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 712
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.037832
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 18.177950


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 713
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.074500
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.923209


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 714
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.022310
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.504950


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 715
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.575724
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.121154


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 716
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.046677
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.438406


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 717
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.136070
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.417667


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 718
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.026924
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.159637


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 719
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.036021
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.637435


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 720
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.167990
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.251499


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 721
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.105659
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.949485


Training loss: 2.302585
Training accuracy: 0.312500

epoch: 722
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.104324
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.374239


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 723
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.673383
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.153700


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 724
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.662549
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.291135


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 725
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.573390
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.370200


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 726
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.454790
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 17.041599


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 727
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.047349
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.558357


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 728
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.048296
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.655487


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 729
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.088454
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.701784


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 730
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.047218
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.839941


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 731
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.080298
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.702205


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 732
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.041048
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.390993


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 733
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.165917
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.628153


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 734
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.057907
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.099630


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 735
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.057176
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 17.759436


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 736
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.442365
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.197248


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 737
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.110398
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.007939


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 738
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.449177
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.422959


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 739
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.052545
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.474606


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 740
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.465816
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.867393


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 741
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.045099
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.098624


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 742
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.453701
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.200850


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 743
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.049772
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.025625


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 744
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.050141
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.012506


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 745
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.120444
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.701531


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 746
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.076170
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.851178


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 747
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.072750
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.822289


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 748
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.469584
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.685385


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 749
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.658763
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.120316


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 750
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.044063
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.124929


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 751
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.044546
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.772068


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 752
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.644867
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.249817


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 753
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.369246
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.229904


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 754
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.462163
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.509764


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 755
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.053025
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.601112


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 756
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -1.144176
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.061267


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 757
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.052010
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.028774


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 758
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -1.118273
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.311399


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 759
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.466245
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.291395


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 760
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.158290
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.159451


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 761
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.041674
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.588700


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 762
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.587598
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.891033


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 763
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.037833
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.441556


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 764
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.189846
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.380769


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 765
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.481986
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.860615


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 766
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.483322
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.299130


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 767
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.027192
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.876438


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 768
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.229250
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.477474


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 769
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.346078
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.110355


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 770
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.015485
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.818366


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 771
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.572672
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.154520


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 772
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.576320
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.579515


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 773
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.485432
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.134759


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 774
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.080830
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.102463


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 775
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.141536
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.076683


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 776
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.032368
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.398077


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 777
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.535434
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.420762


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 778
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.031226
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.580700


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 779
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.178887
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.484113


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 780
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.021439
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.774259


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 781
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.017091
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.018990


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 782
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.510195
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.820089


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 783
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.704656
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.404980


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 784
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.013018
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.961350


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 785
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.008585
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.400230


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 786
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.005320
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.000002


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 787
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.005560
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.490804


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 788
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.716555
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.951350


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 789
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.034582
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.800905


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 790
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.026682
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.261985


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 791
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.578640
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.098686


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 792
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.011283
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.774158


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 793
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.530185
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.804444


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 794
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.033171
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.711037


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 795
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.014450
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.705170


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 796
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.015907
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.124613


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 797
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.033697
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.270082


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 798
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.529088
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.946913


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 799
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.013870
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.082478


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 800
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.529458
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.609102


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 801
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.015834
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.737671


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 802
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.020675
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.489037


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 803
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.529549
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.439333


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 804
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.020462
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.594348


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 805
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.230982
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.909737


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 806
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.048911
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.000526


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 807
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.032393
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.943748


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 808
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.260653
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.768728


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 809
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.040763
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.567062


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 810
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.044400
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.087553


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 811
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.442154
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.199419


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 812
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.533543
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.547998


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 813
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.003963
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.052492


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 814
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.501654
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.712987


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 815
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.028481
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.862131


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 816
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.028836
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.859875


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 817
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.030302
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.739896


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 818
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.524183
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.863434


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 819
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.032928
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.326495


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 820
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.034674
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.842753


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 821
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.054218
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.071507


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 822
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.019158
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.566210


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 823
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.283928
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.307870


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 824
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.016328
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.560758


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 825
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.241760
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 14.181486


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 826
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.013781
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.814205


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 827
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.009315
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.587811


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 828
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.002732
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.508589


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 829
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.359239
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.586397


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 830
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.038981
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.011444


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 831
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.039357
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.907540


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 832
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.556368
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.045561


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 833
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.006671
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.518131


Training loss: 2.302585
Training accuracy: 0.312500

epoch: 834
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.005921
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.158146


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 835
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.234435
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.948840


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 836
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.505936
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.504958


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 837
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.042145
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.349476


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 838
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.044916
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.463634


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 839
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.525007
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.842118


Training loss: 2.302585
Training accuracy: 0.312500

epoch: 840
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.540673
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.999704


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 841
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.525163
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.560246


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 842
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.287491
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.643607


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 843
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.047527
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 15.343312


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 844
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.530789
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.769774


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 845
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.047950
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.129204


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 846
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.006509
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.184148


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 847
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.533305
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.186348


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 848
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.049103
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.264425


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 849
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.030967
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.917866


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 850
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.517608
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.508555


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 851
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.517546
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.069336


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 852
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.002276
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.145508


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 853
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.548351
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.501358


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 854
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.002319
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.299149


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 855
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.398340
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.295240


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 856
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.005362
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.868153


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 857
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.042358
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.213989


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 858
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.006723
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.579733


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 859
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.653118
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.289071


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 860
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.015355
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.151136


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 861
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.055740
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.207272


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 862
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.078852
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.996328


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 863
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.060727
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.823929


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 864
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.913948
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.573512


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 865
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.029629
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.802484


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 866
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.013447
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.290815


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 867
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.054895
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.565157


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 868
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.570544
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.231152


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 869
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.466125
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.913163


Training loss: 2.302585
Training accuracy: 0.312500

epoch: 870
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.062482
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.227167


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 871
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.456686
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.730371


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 872
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.750403
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.109301


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 873
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.279409
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.496870


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 874
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.097789
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 18.164940


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 875
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.572998
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.603022


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 876
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.617910
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.240219


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 877
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.062191
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.952819


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 878
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.269588
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.105906


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 879
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.693564
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 15.909100


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 880
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.073197
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.555738


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 881
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.074803
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.258045


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 882
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.097589
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.343504


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 883
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.302610
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.956654


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 884
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.306446
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.598129


Training loss: 2.302585
Training accuracy: 0.312500

epoch: 885
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.256324
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.592796


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 886
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.571873
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.604560


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 887
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.821211
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.389197


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 888
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.070242
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.669859


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 889
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.091022
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.489635


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 890
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.662337
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.191555


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 891
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.096198
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.473242


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 892
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.450530
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.757263


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 893
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.450900
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.739885


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 894
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.826637
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.634461


Training loss: 2.302585
Training accuracy: 0.312500

epoch: 895
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.045785
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.208651


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 896
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.051386
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.404377


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 897
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -1.247271
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.560286


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 898
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.463823
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.864037


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 899
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.735896
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.868460


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 900
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.086062
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.535444


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 901
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.585353
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.684313


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 902
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.102272
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.481385


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 903
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.546867
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.937277


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 904
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.101010
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.864782


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 905
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.081459
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 13.582164


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 906
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.581480
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.826129


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 907
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.077582
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.534145


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 908
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.267158
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.955209


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 909
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.067567
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.015066


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 910
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.067739
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.739276


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 911
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.482512
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.748935


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 912
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.067780
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.185686


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 913
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.071691
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 17.051304


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 914
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.257698
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.243721


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 915
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.058091
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.183651


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 916
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.580148
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.363371


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 917
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.065712
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.152103


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 918
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.462050
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.674393


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 919
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.049598
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 13.950156


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 920
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.054543
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.906698


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 921
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.572167
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.894234


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 922
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.553634
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.384233


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 923
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.260701
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.978275


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 924
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.567113
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.424086


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 925
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.026114
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.308203


Training loss: 2.302585
Training accuracy: 0.312500

epoch: 926
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.082925
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.053284


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 927
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.569162
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.249821


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 928
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.060682
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.895035


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 929
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.060914
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.270297


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 930
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -1.102229
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.529698


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 931
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.616689
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.092419


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 932
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.082748
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.745989


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 933
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.028163
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.547798


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 934
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.071974
relu backward ...
Time spent in relu_backward: 0.000002 seconds
Layer 0 weight grad [0][0] = 15.414149


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 935
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000018 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.476892
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.814075


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 936
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.073378
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.091292


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 937
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -1.253746
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.456227


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 938
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.094081
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.653002


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 939
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.776174
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.982724


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 940
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.728320
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.408978


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 941
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.490939
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 13.808474


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 942
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.059657
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.579038


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 943
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.026357
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.914229


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 944
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.079731
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.803670


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 945
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.025519
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.032522


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 946
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.669217
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.554322


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 947
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.573767
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.271463


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 948
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.072999
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.362104


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 949
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.038677
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.932556


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 950
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.080483
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 15.745571


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 951
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.783975
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.352922


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 952
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -1.331730
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.231778


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 953
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.084440
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.862520


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 954
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.599980
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.339479


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 955
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.581589
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.467496


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 956
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.650789
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.506705


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 957
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.071869
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.683390


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 958
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.037378
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.920976


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 959
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.287207
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.354266


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 960
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.613895
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.281014


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 961
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.090334
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.231431


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 962
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.094481
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 18.019857


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 963
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.096365
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.324495


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 964
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.097126
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.343193


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 965
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.065234
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.329870


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 966
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.289692
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.866653


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 967
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.090399
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.097542


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 968
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000007 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.723062
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.423740


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 969
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.611892
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.229818


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 970
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.451928
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.222118


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 971
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.098513
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.429406


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 972
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.103679
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.284140


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 973
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.311287
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.263453


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 974
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.111718
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.926193


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 975
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.723264
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.395162


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 976
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.090123
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.742836


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 977
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.630961
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.285563


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 978
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.703136
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.128935


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 979
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.706869
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.297105


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 980
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.135189
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.531585


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 981
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.135787
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.304350


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 982
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.655050
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.604925


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 983
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.138179
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 16.140684


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 984
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.102738
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.886589


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 985
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.144536
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 18.113668


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 986
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.120709
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.242533


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 987
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.891430
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.219876


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 988
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.765757
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.391459


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 989
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.769286
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.142101


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 990
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.915677
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.549565


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 991
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.138590
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 13.258086


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 992
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.343464
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.729050


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 993
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.143979
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.618594


Training loss: 2.302585
Training accuracy: 0.062500

epoch: 994
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.444167
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.056149


Training loss: 2.302585
Training accuracy: 0.187500

epoch: 995
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.146057
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 16.413473


Training loss: 2.302585
Training accuracy: 0.000000

epoch: 996
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = 0.147828
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.429176


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 997
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.403998
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 14.221829


Training loss: 2.302585
Training accuracy: 0.125000

epoch: 998
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -0.404352
relu backward ...
Time spent in relu_backward: 0.000000 seconds
Layer 0 weight grad [0][0] = 13.876266


Training loss: 2.302585
Training accuracy: 0.250000

epoch: 999
remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
remainder_m = 0, remainder_n = 2
remainder_m = 0, remainder_n = 2
Layer 1 weight grad [0][0] = -1.040161
relu backward ...
Time spent in relu_backward: 0.000001 seconds
Layer 0 weight grad [0][0] = 15.059426


Training loss: 2.302585
Training accuracy: 0.000000

saving in file: /home/rickojn/coding/cf-mlp/models/model_20260125_163328_h8.mdl
Error opening file /home/rickojn/coding/cf-mlp/models/remainder_m = 0, remainder_n = 2
softmax forward ...
Time spent in softmax_forward: 0.000531 seconds
Test loss after training: 2.301268
Test accuracy after training: 0.098000
