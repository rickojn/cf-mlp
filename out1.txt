rows: 28, cols: 28
Number of training images: 60000
rows: 28, cols: 28
Number of test images: 10000
model layers = 0
Model created with 2 layers
Layer 0: 784 inputs, 8 neurons
Layer 1: 8 inputs, 10 neurons
Number of parameters: 6370
Batch size: 16

Loading model from file /home/patrick/coding/cf-mlp/models//model_20260113_222457_h8.mdl

Model loaded successfully
matmul simd ....
Time spent in matmul_simd_forward: 0.133577 seconds
relu forward ...
Time spent in relu_forward: 0.000192 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.001966 seconds
softmax forward ...
Time spent in softmax_forward: 0.000608 seconds
Test loss before training: 2.332098
Test accuracy before training: 0.099400


training loop:

epoch: 0
matmul simd ....
Time spent in matmul_simd_forward: 0.000101 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.004811
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000090 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.301419
Training accuracy: 0.062500

epoch: 1
matmul simd ....
Time spent in matmul_simd_forward: 0.000115 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000074 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.335348
Training accuracy: 0.062500

epoch: 2
matmul simd ....
Time spent in matmul_simd_forward: 0.000097 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004554
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000074 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.330003
Training accuracy: 0.125000

epoch: 3
matmul simd ....
Time spent in matmul_simd_forward: 0.000095 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.002852
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000074 seconds
Layer 0 weight grad [0][0] = -0.003029


Training loss: 2.315126
Training accuracy: 0.125000

epoch: 4
matmul simd ....
Time spent in matmul_simd_forward: 0.000101 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.012137
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000074 seconds
Layer 0 weight grad [0][0] = 0.042260


Training loss: 2.339449
Training accuracy: 0.062500

epoch: 5
matmul simd ....
Time spent in matmul_simd_forward: 0.000099 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.002198
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000080 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.354102
Training accuracy: 0.062500

epoch: 6
matmul simd ....
Time spent in matmul_simd_forward: 0.000103 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000089 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.354132
Training accuracy: 0.062500

epoch: 7
matmul simd ....
Time spent in matmul_simd_forward: 0.000118 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000091 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.271854
Training accuracy: 0.062500

epoch: 8
matmul simd ....
Time spent in matmul_simd_forward: 0.000121 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004891
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000095 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.237350
Training accuracy: 0.375000

epoch: 9
matmul simd ....
Time spent in matmul_simd_forward: 0.000110 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000079 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.323307
Training accuracy: 0.062500

epoch: 10
matmul simd ....
Time spent in matmul_simd_forward: 0.000099 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000074 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.305055
Training accuracy: 0.000000

epoch: 11
matmul simd ....
Time spent in matmul_simd_forward: 0.000098 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000074 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.409055
Training accuracy: 0.000000

epoch: 12
matmul simd ....
Time spent in matmul_simd_forward: 0.000101 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000352
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000083 seconds
Layer 0 weight grad [0][0] = 0.000864


Training loss: 2.342040
Training accuracy: 0.062500

epoch: 13
matmul simd ....
Time spent in matmul_simd_forward: 0.000098 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.004256
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000074 seconds
Layer 0 weight grad [0][0] = -0.010426


Training loss: 2.266505
Training accuracy: 0.187500

epoch: 14
matmul simd ....
Time spent in matmul_simd_forward: 0.000099 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000004 seconds
softmax forward ...
Time spent in softmax_forward: 0.000001 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000770
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000087 seconds
Layer 0 weight grad [0][0] = -0.000553


Training loss: 2.297504
Training accuracy: 0.062500

epoch: 15
matmul simd ....
Time spent in matmul_simd_forward: 0.000108 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000084 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.324391
Training accuracy: 0.062500

epoch: 16
matmul simd ....
Time spent in matmul_simd_forward: 0.000135 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000080 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.322954
Training accuracy: 0.062500

epoch: 17
matmul simd ....
Time spent in matmul_simd_forward: 0.000096 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000071 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.302938
Training accuracy: 0.125000

epoch: 18
matmul simd ....
Time spent in matmul_simd_forward: 0.000137 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000142 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.240916
Training accuracy: 0.250000

epoch: 19
matmul simd ....
Time spent in matmul_simd_forward: 0.000145 seconds
relu forward ...
Time spent in relu_forward: 0.000004 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000141 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.294249
Training accuracy: 0.125000

epoch: 20
matmul simd ....
Time spent in matmul_simd_forward: 0.000163 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000140 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.373177
Training accuracy: 0.000000

epoch: 21
matmul simd ....
Time spent in matmul_simd_forward: 0.000161 seconds
relu forward ...
Time spent in relu_forward: 0.000004 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.004862
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000126 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.317384
Training accuracy: 0.062500

epoch: 22
matmul simd ....
Time spent in matmul_simd_forward: 0.000111 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000006 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.002915
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000144 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.339133
Training accuracy: 0.062500

epoch: 23
matmul simd ....
Time spent in matmul_simd_forward: 0.000105 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000005 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.001976
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000087 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.233268
Training accuracy: 0.312500

epoch: 24
matmul simd ....
Time spent in matmul_simd_forward: 0.000164 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000155 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.303987
Training accuracy: 0.125000

epoch: 25
matmul simd ....
Time spent in matmul_simd_forward: 0.000183 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000164 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.297861
Training accuracy: 0.062500

epoch: 26
matmul simd ....
Time spent in matmul_simd_forward: 0.000195 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.002954
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000144 seconds
Layer 0 weight grad [0][0] = -0.002370


Training loss: 2.240431
Training accuracy: 0.250000

epoch: 27
matmul simd ....
Time spent in matmul_simd_forward: 0.000187 seconds
relu forward ...
Time spent in relu_forward: 0.000004 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000009 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.001219
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000147 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.299151
Training accuracy: 0.125000

epoch: 28
matmul simd ....
Time spent in matmul_simd_forward: 0.000181 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000141 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.398536
Training accuracy: 0.000000

epoch: 29
matmul simd ....
Time spent in matmul_simd_forward: 0.000175 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000142 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.326157
Training accuracy: 0.000000

epoch: 30
matmul simd ....
Time spent in matmul_simd_forward: 0.000211 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.004520
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000159 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.277692
Training accuracy: 0.062500

epoch: 31
matmul simd ....
Time spent in matmul_simd_forward: 0.000187 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.004498
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000151 seconds
Layer 0 weight grad [0][0] = -0.005030


Training loss: 2.360697
Training accuracy: 0.187500

epoch: 32
matmul simd ....
Time spent in matmul_simd_forward: 0.000187 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000022 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000146 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.353955
Training accuracy: 0.062500

epoch: 33
matmul simd ....
Time spent in matmul_simd_forward: 0.000183 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000142 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.351663
Training accuracy: 0.000000

epoch: 34
matmul simd ....
Time spent in matmul_simd_forward: 0.000175 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000142 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.342755
Training accuracy: 0.000000

epoch: 35
matmul simd ....
Time spent in matmul_simd_forward: 0.000175 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000006 seconds
Layer 1 weight grad [0][0] = 0.005015
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000142 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.264910
Training accuracy: 0.187500

epoch: 36
matmul simd ....
Time spent in matmul_simd_forward: 0.000186 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000141 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.353065
Training accuracy: 0.062500

epoch: 37
matmul simd ....
Time spent in matmul_simd_forward: 0.000170 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.001000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000163 seconds
Layer 0 weight grad [0][0] = -0.000834


Training loss: 2.343923
Training accuracy: 0.062500

epoch: 38
matmul simd ....
Time spent in matmul_simd_forward: 0.000192 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000151 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.218781
Training accuracy: 0.250000

epoch: 39
matmul simd ....
Time spent in matmul_simd_forward: 0.000167 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.003610
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000151 seconds
Layer 0 weight grad [0][0] = -0.002651


Training loss: 2.210988
Training accuracy: 0.250000

epoch: 40
matmul simd ....
Time spent in matmul_simd_forward: 0.000175 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000142 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.308142
Training accuracy: 0.187500

epoch: 41
matmul simd ....
Time spent in matmul_simd_forward: 0.000177 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000142 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.315360
Training accuracy: 0.000000

epoch: 42
matmul simd ....
Time spent in matmul_simd_forward: 0.000162 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000142 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.326864
Training accuracy: 0.062500

epoch: 43
matmul simd ....
Time spent in matmul_simd_forward: 0.000185 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000145 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.246705
Training accuracy: 0.187500

epoch: 44
matmul simd ....
Time spent in matmul_simd_forward: 0.000179 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000146 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.307722
Training accuracy: 0.062500

epoch: 45
matmul simd ....
Time spent in matmul_simd_forward: 0.000183 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000148 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.313935
Training accuracy: 0.062500

epoch: 46
matmul simd ....
Time spent in matmul_simd_forward: 0.000179 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.006169
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000143 seconds
Layer 0 weight grad [0][0] = -0.007681


Training loss: 2.296350
Training accuracy: 0.187500

epoch: 47
matmul simd ....
Time spent in matmul_simd_forward: 0.000171 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000143 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.393025
Training accuracy: 0.000000

epoch: 48
matmul simd ....
Time spent in matmul_simd_forward: 0.000189 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000144 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.322455
Training accuracy: 0.062500

epoch: 49
matmul simd ....
Time spent in matmul_simd_forward: 0.000176 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000006 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000145 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.344235
Training accuracy: 0.000000

epoch: 50
matmul simd ....
Time spent in matmul_simd_forward: 0.000181 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000148 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.312495
Training accuracy: 0.062500

epoch: 51
matmul simd ....
Time spent in matmul_simd_forward: 0.000179 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000143 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.294398
Training accuracy: 0.187500

epoch: 52
matmul simd ....
Time spent in matmul_simd_forward: 0.000179 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.001760
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000144 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.235669
Training accuracy: 0.250000

epoch: 53
matmul simd ....
Time spent in matmul_simd_forward: 0.000180 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = -0.030030
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000141 seconds
Layer 0 weight grad [0][0] = 0.012941


Training loss: 2.322198
Training accuracy: 0.125000

epoch: 54
matmul simd ....
Time spent in matmul_simd_forward: 0.000182 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000118 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.294945
Training accuracy: 0.062500

epoch: 55
matmul simd ....
Time spent in matmul_simd_forward: 0.000177 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000006 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000138 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.268764
Training accuracy: 0.187500

epoch: 56
matmul simd ....
Time spent in matmul_simd_forward: 0.000186 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000143 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.320729
Training accuracy: 0.062500

epoch: 57
matmul simd ....
Time spent in matmul_simd_forward: 0.000201 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000141 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.327276
Training accuracy: 0.000000

epoch: 58
matmul simd ....
Time spent in matmul_simd_forward: 0.000183 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000014 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000142 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.311491
Training accuracy: 0.187500

epoch: 59
matmul simd ....
Time spent in matmul_simd_forward: 0.000175 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.004411
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000142 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.299727
Training accuracy: 0.250000

epoch: 60
matmul simd ....
Time spent in matmul_simd_forward: 0.000177 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.003834
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000144 seconds
Layer 0 weight grad [0][0] = -0.019910


Training loss: 2.307347
Training accuracy: 0.187500

epoch: 61
matmul simd ....
Time spent in matmul_simd_forward: 0.000181 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000135 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.331974
Training accuracy: 0.000000

epoch: 62
matmul simd ....
Time spent in matmul_simd_forward: 0.000181 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000147 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.291110
Training accuracy: 0.062500

epoch: 63
matmul simd ....
Time spent in matmul_simd_forward: 0.000192 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000009 seconds
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000141 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.296792
Training accuracy: 0.187500

epoch: 64
matmul simd ....
Time spent in matmul_simd_forward: 0.000187 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000146 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.308017
Training accuracy: 0.125000

epoch: 65
matmul simd ....
Time spent in matmul_simd_forward: 0.000185 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000149 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.321339
Training accuracy: 0.187500

epoch: 66
matmul simd ....
Time spent in matmul_simd_forward: 0.000215 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000146 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.310855
Training accuracy: 0.062500

epoch: 67
matmul simd ....
Time spent in matmul_simd_forward: 0.000203 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000009 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000143
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000122 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.260051
Training accuracy: 0.187500

epoch: 68
matmul simd ....
Time spent in matmul_simd_forward: 0.000189 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000144 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.286431
Training accuracy: 0.250000

epoch: 69
matmul simd ....
Time spent in matmul_simd_forward: 0.000194 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000145 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.329306
Training accuracy: 0.062500

epoch: 70
matmul simd ....
Time spent in matmul_simd_forward: 0.000174 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000009 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000158 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.332263
Training accuracy: 0.000000

epoch: 71
matmul simd ....
Time spent in matmul_simd_forward: 0.000175 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000140 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.293448
Training accuracy: 0.062500

epoch: 72
matmul simd ....
Time spent in matmul_simd_forward: 0.000188 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.004678
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000143 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.276285
Training accuracy: 0.250000

epoch: 73
matmul simd ....
Time spent in matmul_simd_forward: 0.000193 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000142 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.321142
Training accuracy: 0.062500

epoch: 74
matmul simd ....
Time spent in matmul_simd_forward: 0.000178 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000143 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.308836
Training accuracy: 0.125000

epoch: 75
matmul simd ....
Time spent in matmul_simd_forward: 0.000160 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.003398
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000155 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.314783
Training accuracy: 0.062500

epoch: 76
matmul simd ....
Time spent in matmul_simd_forward: 0.000183 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000140 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.331950
Training accuracy: 0.000000

epoch: 77
matmul simd ....
Time spent in matmul_simd_forward: 0.000174 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.003137
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000141 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.335714
Training accuracy: 0.000000

epoch: 78
matmul simd ....
Time spent in matmul_simd_forward: 0.000183 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000014 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000142 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.309036
Training accuracy: 0.062500

epoch: 79
matmul simd ....
Time spent in matmul_simd_forward: 0.000167 seconds
relu forward ...
Time spent in relu_forward: 0.000003 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000103 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.314875
Training accuracy: 0.062500

epoch: 80
matmul simd ....
Time spent in matmul_simd_forward: 0.000163 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000146 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.324766
Training accuracy: 0.062500

epoch: 81
matmul simd ....
Time spent in matmul_simd_forward: 0.000186 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.001839
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000145 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.348942
Training accuracy: 0.000000

epoch: 82
matmul simd ....
Time spent in matmul_simd_forward: 0.000180 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.004360
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000143 seconds
Layer 0 weight grad [0][0] = -0.005247


Training loss: 2.287811
Training accuracy: 0.125000

epoch: 83
matmul simd ....
Time spent in matmul_simd_forward: 0.000175 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.004131
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000148 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.356900
Training accuracy: 0.000000

epoch: 84
matmul simd ....
Time spent in matmul_simd_forward: 0.000184 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000137 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.307332
Training accuracy: 0.125000

epoch: 85
matmul simd ....
Time spent in matmul_simd_forward: 0.000167 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000005 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000003 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000110 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.318179
Training accuracy: 0.000000

epoch: 86
matmul simd ....
Time spent in matmul_simd_forward: 0.000189 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000146 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.251042
Training accuracy: 0.125000

epoch: 87
matmul simd ....
Time spent in matmul_simd_forward: 0.000178 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.010155
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000142 seconds
Layer 0 weight grad [0][0] = -0.030224


Training loss: 2.342505
Training accuracy: 0.125000

epoch: 88
matmul simd ....
Time spent in matmul_simd_forward: 0.000181 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000143 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.315227
Training accuracy: 0.000000

epoch: 89
matmul simd ....
Time spent in matmul_simd_forward: 0.000177 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000142 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.270917
Training accuracy: 0.125000

epoch: 90
matmul simd ....
Time spent in matmul_simd_forward: 0.000179 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.003956
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000142 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.325721
Training accuracy: 0.000000

epoch: 91
matmul simd ....
Time spent in matmul_simd_forward: 0.000180 seconds
relu forward ...
Time spent in relu_forward: 0.000001 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000004 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000145 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.296537
Training accuracy: 0.187500

epoch: 92
matmul simd ....
Time spent in matmul_simd_forward: 0.000179 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000004 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.001889
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000143 seconds
Layer 0 weight grad [0][0] = 0.005236


Training loss: 2.308852
Training accuracy: 0.187500

epoch: 93
matmul simd ....
Time spent in matmul_simd_forward: 0.000177 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000007 seconds
softmax forward ...
Time spent in softmax_forward: 0.000003 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000001 seconds
matmul backward ...
Time spent in matmul_backward: 0.000143 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.325006
Training accuracy: 0.000000

epoch: 94
matmul simd ....
Time spent in matmul_simd_forward: 0.000182 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in softmax_forward: 0.000002 seconds
loss softmax backward ...
Time spent in loss_softmax_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000005 seconds
Layer 1 weight grad [0][0] = 0.000000
relu backward ...
Time spent in relu_backward: 0.000002 seconds
matmul backward ...
Time spent in matmul_backward: 0.000142 seconds
Layer 0 weight grad [0][0] = 0.000000


Training loss: 2.288461
Training accuracy: 0.125000

epoch: 95
matmul simd ....
Time spent in matmul_simd_forward: 0.000177 seconds
relu forward ...
Time spent in relu_forward: 0.000002 seconds
matmul simd ....
remainder_m = 0, remainder_n = 2
Time spent in matmul_simd_forward: 0.000008 seconds
softmax forward ...
Time spent in sof